% !TEX encoding = IsoLatin

% La riga soprastante serve per configurare gli editor TeXShop, TeXWorks
% e TeXstudio per gestire questo file con la codifica IsoLatin o Latin 1
% o ISO 8859-1.

% per commentare una riga mettere % al suo inizio
% per s-commentare una riga (ossia attivarla) togliere il % al suo inizio
%
\documentclass[pdfa% formato PDF/A, obbligatorio per l'archiviazione delle tesi di Polito
,cucitura%lascia margine per la rilegatura
%,twoside% per stampa fronte-retro (fortemente consigliato per tesi voluminose, opzionale per le altre)
%,12pt% font più grande (12pt) rispetto a quello normalmente usato (11pt)
]{toptesi}
%
\usepackage{hyperref}
\hypersetup{%
    pdfpagemode={UseOutlines},
    bookmarksopen,
    pdfstartview={FitH},
    colorlinks,
    linkcolor={blue},
    citecolor={red},
    urlcolor={blue}
  }
% \documentclass[11pt,twoside,oldstyle,autoretitolo,classica,greek]{toptesi}
% \usepackage[or]{teubner}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Esempio di composizione di tesi di laurea.
%
% Questo esempio e' stato preparato inizialmente 13-marzo-1989
% e poi e' stato modificato via via che TOPtesi andava
% arricchendosi di altre possibilita'.
%
% Nel seguito laurea "quinquennale" sta anche per "specialistica" o "magistrale"

% Cambiare encoding a piacere; oppure non caricare nessun encoding se si usano
% solo caratteri a 7 bit (ASCII) nei file d'entrata.
%
\usepackage[latin1]{inputenc}% IMPORTANTE! usare codifica ISO-8859-1 per le lettere accentate

% temporaneo
\usepackage[dvipsnames]{xcolor}

\input{commands.tex}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
	backgroundcolor=\color{backcolour},   
	commentstyle=\color{codegreen},
	keywordstyle=\color{blue},
	numberstyle=\tiny\color{codegray},
	stringstyle=\color{codepurple},
	basicstyle=\ttfamily\footnotesize,
	breakatwhitespace=false,         
	breaklines=true,                 
	captionpos=b,                    
	keepspaces=true,                 
	numbers=left,                    
	numbersep=5pt,                  
	showspaces=false,                
	showstringspaces=false,
	showtabs=false,                  
	tabsize=2
}

\usepackage{xcolor}

\begin{document}
	
\selectlanguage{english}

\ateneo{Politecnico di Torino}

%%% scegliere la propria facoltà (solo PRIMA dell'AA 2012-2013)
%
%\facolta[III]{Ingegneria dell'Informazione}
%\facolta[IV]{Organizzazione d'Impresa\\e Ingegneria Gestionale}
%\Materia{Remote sensing}% uso sconsigliato

%\monografia{Gestione informatizzata di un magazzino ricambi}% per la laurea triennale
\titolo{Automatic Malware Signature Generation}% per la laurea quinquennale e il dottorato
%\sottotitolo{Metodo dei satelliti medicei}% NON obbligatorio, per la laurea quinquennale e il dottorato

%%% scegliere il proprio corso
%
%\corsodilaurea{Ingegneria dell'Organizzazione d'Impresa}% per la laurea di primo e secondo livello
%\corsodilaurea{Ingegneria Logistica e della Produzione}% per la laurea di primo e secondo livello
%\corsodilaurea{Ingegneria Gestionale}% per la laurea di primo e secondo livello
\corsodilaurea{Ingegneria Informatica}% per la laurea di primo e secondo livello
%\corsodidottorato{Meccanica}% per il dottorato

\candidato{Michele \textsc{Crepaldi}}% per tutti i percorsi
%\secondocandidato{Evangelista \textsc{Torricelli}}% per la laurea magistrale solamente
%\direttore{prof. Albert Einstein}% per il dottorato
%\coordinatore{prof. Albert Einstein}% per il dottorato
\relatore{prof.\ Antonio Lioy}% per la laurea e il dottorato
\secondorelatore{ing.~Andrea Atzeni}% per la laurea magistrale
%\terzorelatore{{\tabular{@{}l}dott.\ Neil Armstrong\\prof. Maria Rossi\endtabular}}% per la laurea magistrale
%\tutore{ing.~Andrea Atzeni}% per il dottorato
%\tutoreaziendale{dott.\ ing.\ Giovanni Giacosa} % solo per la laurea di secondo livello con tesi svolta in azienda
%\NomeTutoreAziendale{Supervisore aziendale\\Centro Ricerche FIAT}
%\sedutadilaurea{Agosto 1615}% per la laurea quinquennale
%\esamedidottorato{Novembre 1610}% per il dottorato
%\sedutadilaurea{\textsc{Novembre} 2017}% per la laurea triennale
\sedutadilaurea{\textsc{Anno~accademico} 2020-2021}% per la laurea magistrale
%\annoaccademico{1615-1616}% solo con l'opzione classica
%\annoaccademico{2006-2007}% idem
%\ciclodidottorato{XV}% per il dottorato
\logosede{logopolito}
%
%\chapterbib %solo per vedere che cosa succede; e' preferibile comporre una sola bibliografia
%\AdvisorName{Supervisors}
%\newtheorem{osservazione}{Osservazione}% Standard LaTeX

%\usepackage[a-1b]{pdfx}
%\hypersetup{%
%    pdfpagemode={UseOutlines},
%    bookmarksopen,
%    pdfstartview={FitH},
%    colorlinks,
%    linkcolor={blue},
%    citecolor={green},
%    urlcolor={blue}
%  }

%
% per numerare e far comparire nell'indice anche le sezioni di quarto livello
% SCONSIGLIATO! da usarsi solo in caso di estrema necessità
%\setcounter{secnumdepth}{4}% section-numbering-depth
%\setcounter{tocdepth}{4}% TOC-numbering-depth (TOC=Table-Of-Content)

%\setbindingcorrection{3mm}

\errorcontextlines=9

\frontespizio
\paginavuota
\newpage
%per sfruttare meglio lo spazio nella pagina
\advance\voffset -5mm
\advance\textheight 30mm

% opzionale, solo se si vuole dedicare la tesi a delle persone care
\begin{dedica}
	
Thanks...

\end{dedica}

\sommario

Summary...

\ringraziamenti

Aknowledgments...

%% inserire sempre nella tesi per la laurea di I livello, perché il nome dei tutori non è indicato sul frontespizio.
%Il lavoro descritto in questa monografia è stato svolto sotto la supervisione
%del Prof. Antonio Lioy (tutore accademico)% inserire sempre il nome del tutore accademico
% e dell'Ing. Mario Rossi (tutore aziendale)% inserire solo se la monografia è relativa ad un tirocinio.
%.

%\tablespagetrue % normalmente questa riga non serve ed e' commentata
%\figurespagetrue % normalmente questa riga non serve ed e' commentata

\indici

\mainmatter

\chapter{Introduction}

\color{Red}
The accelerating rate of malware incidents on daily basis indicates the magnitude of the problem in malware analysis. While malware analysts detect many malware attacks and incidents, keeping pace with the number and different types of attacks poses a significant challenge to malware analysts. There is no silver bullet with respect to malware, as there is no single malware analysis technique with the capability to treat all malware incidents, as a result analysts select the most suitable malware analysis technique for the specific security incident under consideration \cite{NaikEAGYRETE}.
\color{Black}

\chapter{Background}

\section{Malware}
\textbf{Malware}, short for \textbf{\textit{malicious software}}, is a general term for all types of programs designed to perform harmful or undesirable actions on a system. In fact in the context of IT security the term \textit{malicious software} commonly means \cite{SharpAIM}:

\begin{quote}
	\textit{Software which is used with the aim of attempting to breach a computer system's security policy with respect to Confidentiality, Integrity and/or Availability.}
\end{quote}

Malware consists of programming artefacts (code, scripts, active content, and other software) designed to disrupt or deny operation, gain unauthorized access to system resources, gather information that leads to loss of privacy or exploitation, and other abusive behaviour.
Malware is not (and should not be confused with) defective software - software that has a legitimate purpose but contains harmful bugs (programming errors).

Different companies, organizations and people describe malware in various ways. For example \textbf{Microsoft} defines it in a generic way as:

\begin{quote}
	\textit{Malware is a catch-all term to refer to any software designed to cause damage to a single computer, server, or computer network} \cite{MoirDM}.
\end{quote}

The \textbf{National Institute of Standards and Technology} (\textbf{NIST}), on the other hand, presents multiple definitions for malware, describing it as "hardware, firmware, or software that is intentionally included or inserted in a system for a harmful purpose" \cite{nistM}.

In another more specific definition \textbf{NIST} affirms that Malware is:

\begin{quote}
	\textit{A program that is inserted into a system, usually covertly, with the intent of compromising the confidentiality, integrity, or availability of the victim's data, applications, or operating system or of otherwise annoying or disrupting the victim} \cite{nistM}.
\end{quote}

Notice that, since the attacker can use a number of different means - such as executable code, interpreted code, scripts, macros etc. - to perpetrate its malicious intents, the term \textbf{\textit{software}} should be understood in a broader sense in the above definitions.\\
Moreover, the computer system whose security policy is attempted to be breached is usually known as the \textbf{\textit{target}} for the malware. Instead, the cybercriminal who originally launched the malware with the purpose of attacking one or more targets is generally referred to as the "\textit{initiator} of the malware". Furthermore, depending on the malware type, the initiator may or may not exactly know what the set of targets is \cite{SharpAIM}.\\
According to the above definitions software is defined as malicious in relation to an attempted breach of the target's \textbf{\textit{security policy}}.
In other words, software is often identified as malware based on its \textit{intended use}, rather than the particular technique or technology used to build it.

\subsection{Why is Malware used}
Generally, cybercriminals use malware to access targets' sensitive data, extort ransoms, or simply cause as much damage as possible to the affected systems.

More generally malware serves a variety of purposes. For example, the most common cybercriminals' uses of malware are: \cite{CraneWIM}

\begin{itemize}
	\item \textbf{To profit financially (either directly or through the sale of their products or services)}.
	For example, attackers may use malware to infect targets' devices with the purpose of stealing their credit account information or cryptocurrency. Alternatively, they may sell their malware to other cybercriminals or as a service offering (\textit{malware-as-a-service}).
	
	\item \textbf{As a means of revenge or to carry out a personal agenda}.
	For example, Brian Krebs of Krebs on Security was struck by a big DDoS attack in 2016 after having talked about a DDoS attacker on his blog.
	
	\item \textbf{To carry out a political or social agenda}.
	Nation-state actors (like state-run hacker groups in China and North Korea) and hacker groups such as Anonymous are a perfect example.
	
	\item \textbf{As a way to entertain themselves}.
	Some cybercriminals enjoy victimizing others.
\end{itemize}

Obviously there are also reasons for non-malicious actors to create and/or deploy some types of malware too - for example it can be used to test a system's security.

\subsection{Malware types}
There are numerous different ways of categorizing malware; one way is by \textit{how} the malicious software spreads. Another one is by what it \textit{does} once it has successfully infected its victim's computers (i.e. what is its payload, how it exploits or makes the system vulnerable).

\subsubsection{By how they spread}
Terms like \textit{trojan}, \textit{virus} and \textit{worm} are commonly used interchangeably to indicate generic malware, but they actually describe three subtly different ways malware can infect target computers \cite{SymantecDVWT}:

\begin{itemize}
	\item \textbf{\textit{Trojan horse}}. Generally speaking, a \textit{Trojan Horse}, commonly referred to as a "Trojan", is any program that disguises itself as legitimate and invites the user to download and run it, concealing a malicious payload. When executed, the payload - malicious routines - may immediately take effect and cause many undesirable effects, such as deleting the user files or installing additional malware or PUAs (Potentially Unwanted Apps).
	
	Trojans known as \textit{droppers} are often used to start a worm outbreak, by injecting the worm into users' local networks \cite{MullinsMT}.
	
	Trojans may hide in games, apps, or even software patches, or they may rely on social engineering and be embedded in attachments included in phishing emails.
	
	Trojan horses cannot self-replicate. They rely on the system operators to activate. However, they can grant the attacker remote access permitting him to then perform any malicious activity that is in their interest. Trojan horse programs can affect the host in many different ways, depending on the payload attached to them \cite{NamanyaTWM}.
	
	\item \textbf{\textit{Virus}}. The term "computer virus" is used for describing a passive self-replicating malicious program. Usually spread via infected websites, file sharing, or email attachment downloads, it will lie dormant until the infected host file or program is activated. At that point it spreads to other executables (and/or boot sectors) by embedding copies of itself into those files. A virus, in fact, in order to spread from one computer to another, usually relies on the infected files possibly ending up, by some means or another, in the target system. Viruses are therefore passive. The mean of transport (file, media file, network file, etc.) is often referred to as the virus \textit{vector}. Depending on how complex the virus code is, it may be able to modify its copies upon replication. For the transport of the infected files to the target system(s), the virus may rely on an unsuspecting human user (who for example uses a USB drive containing the infected file) or initiate itself the transfer (for example, it may send the infected files as an e-mail attachment) \cite{SharpAIM}.
	
	Viruses may also perform other harmful actions other than just replicating, such as creating a backdoor for later use, damaging files, stealing information, creating botnets, render advertisements or even damaging equipment.
	
	\item \textbf{\textit{Worm}}. On the other hand, a worm is a self-replicating, active malicious program that exploits various system vulnerabilities to spread over the network. Particularly, it relies on vulnerabilities present in the target's operating system or installed software. Worms usually consume a lot of bandwidth and processing resources due to continuous scanning and may render the host unstable, sometimes causing the system to crash. Computer worms may also contain "payloads" to damage the target systems. Payloads are pieces of code written to perform various nefarious actions on the affected computers among which stealing data, deleting files or creating bots - which can lead the infected systems to become part of a botnet \cite{NamanyaTWM}.
	
\end{itemize}

These definitions lead to the observation that viruses require \textit{user intervention} to spread, whereas a worm spreads itself automatically. A virus, however, cannot execute or reproduce unless the application it infected is running. This dependence on a host program makes viruses different from trojans, which require users to download them, and worms, which do not use applications to execute.

Furthermore, attackers can also install malware "manually" on a computer, either by gaining physical access to the target system or by using privilege escalation methods to obtain remote administrator access \cite{FruhlingerME}.

\subsubsection{By what they do}
There are a wide range of potential attack techniques used by malware, here are some of them:

\begin{itemize}
	\item \textbf{\textit{Adware}}. \textit{Adware}, or "Advertising supported software", is any software package which automatically plays, displays, or downloads advertisements to a computer. Some adware may also re-direct the user's browser to dubious websites. These advertisements can be in the form of a pop-up ads or ad banners in websites, or advertisements displayed by software, that lure the user into making a purchase. The goal of Adware is to generate revenue for its author.
	
	Often times software and application authors offer "free", or discounted, versions of their creations that come bundled with adware. Adware, in fact, is usually seen by the developers as a way to recover development costs. The income derived from ads may motivate the developer to continue developing, maintaining and upgrading his software product. On the other hand users may see advertisements as annoyances, interruptions, or as distractions from the task at hand \cite{MullinsMT}.
	
	Adware, by itself, is annoying but somewhat harmless, since it is solely designed to deliver ads; however, adware often comes bundled with spyware (such as keyloggers), and/or other privacy-invasive software that is capable of tracking user activity and steal information. Adware-spyware bundles are therefore much more dangerous then adware on its own \cite{DuPaulCMT}.
	
	\item \textbf{\textit{Backdoor}}. A \textit{backdoor}, also called Remote Access Trojan (RAT), is a vulnerability deliberately buried into software's code that allows to bypass typical protection mechanisms, like credentials-based login authentication. In other words, it is a method of circumventing normal authentication procedures. Once a system has been compromised (by others types of malware or other methods), one or more backdoors may be installed. This is done with the purpose of allowing the attacker easier access in the future without alerting the user or the system's security programs. Moreover, backdoors may also be installed before other malicious software, to allow attackers entry \cite{MullinsMT}.
	
	Many device or software manufacturers ship their products with intentionally hidden backdoors to allow company personnel or law enforcement to access the system when needed \cite{IngallsTOM}. Alternatively, backdoors are sometimes hidden in programs also by intelligence services. For example, Cisco network routers, which process large volumes of global internet traffic, in the past were equipped with backdoors intended for US Secret Service use \cite{MyraSecurityWIM}.
	
	However, when used by malicious actors, backdoors grant access to attackers without the user knowledge, thus putting the system in real danger.
	
	\item \textbf{\textit{Browser Hijacker}}. A \textit{Browser Hijacker}, also called "hijackware", is a type of malicious program which considerably modifies the behaviour of the victim's web browser. For example it can force the browser to send the user to a new search page, slow down the loading, change the victim's home page, install unwanted toolbars, redirect the user to specific sites, and display unwanted ads without the user consent.\\
	It can be used to make money off ads, to steal information from users, or to infect the systems with other malware by redirecting users to malicious websites \cite{IngallsTOM}.
	
	\item \textbf{\textit{Bots}/\textit{Botnet}}. In general, \textit{bots} (short for 'robots') are software programs designed to automatically perform specific operations. Bots were originally developed to programmatically manage chat IRC channels - Internet Relay Chat: a text-based communication protocol appeared in 1989.\\
	Some bots are still being used for legitimate and harmless purposes such as video programming, video gaming, internet auctions and online contest, among other functions. It is however becoming increasingly common to see bots being used maliciously. Malicious bots can be (and usually are) used to form botnets. A botnet is defined as a network of host computers (zombies/bots) that is controlled by an attacker - the \textit{bot-master} \cite{NamanyaTWM}. Botnets are frequently used for DDoS (Distributed Denial of Service) attacks, but there are other ways that botnets can be useful to cybercriminals: \cite{CraneWIM}
	
	\begin{itemize}
		\item \textbf{Brute force \& credential stuffing} - Bots can be used to carry out different types of brute force attacks on websites. For example they can use a pre-configured list of usernames and passwords combinations on website login pages with the hope of finding a winning combination, after enough tries.
		
		\item \textbf{Data and content scraping} - Botnets can be used as web spiders to scour websites and databases to gather useful information - such as site content, pricing sheets, etc. - which can be used to obtain an unfair advantage against the competition.
		
		\item \textbf{Botnet-as-a-service opportunities} - Botnets are sometimes rented out by their creators to all kinds of malicious users - including less tech-savvy ones. Doing so, even inexperienced attackers can carry out attacks, such as taking down a target's servers and networks with a DDoS, using these mercenary bots. This service model is sometimes called malware-as-a-service.
		
		\item \textbf{Spambot} - A botnet can also be used to act as a spambot and render advertisements on websites.
		
		\item \textbf{Malware distributor} - Finally Botnets can even be used for distributing malware disguised as popular search items on download sites.
	\end{itemize}
	
	\item \textbf{\textit{Crypto-miner}}. Crypto-miners are a relatively new family of malware. Cybercriminals employ this type of malicious tools to mine Bitcoin and/or other bitcoin-alike digital currencies on the target machine. The victim system's computing power is used for this, without the owner realising it. The mined coins end up in the attackers' digital crypto wallets.
	
	Recently, a more modern method of crypto-mining that works within browsers (also called crypto-jacking), has become quite popular.
	
	In some cases, the use of crypto-miners may be deemed legal. For example they could be used to monetize websites, granted that the site operator clearly informed visitors of the use of such tools \cite{MyraSecurityWIM}.
	
	Finally, according to ESET, most crypto-miners focus mostly on \textit{Monero} as target crypto-currency because it offers anonymous transactions and can be mined with regular CPUs and GPUs instead of expensive, specialized hardware \cite{CraneWIM}.
	
	\item \textbf{\textit{File-less malware}}. File-less malware is a type of memory-resident malware that uses legitimate code already existing within the target computer or device to carry out attacks. As the term suggests, it is malware that operates from a victim's computer memory, not from files on the hard drive, taking advantage of legitimate tools and software (known as "LOLBins" \cite{CraneWIM}) that already exist within the system. File-less malware attacks leave no malware files to scan and no malicious processes to detect. Since there are no files to scan, it is harder to detect and remove than traditional malware; this makes them up to ten times more successful than traditional malware attacks \cite{BakerMCTM}. Furthermore, it also renders forensics more difficult because when the victim's computer is rebooted the malware disappears.
	
	\item \textbf{\textit{Keylogger}}. Keystroke logging (often called \textit{keylogging}) is the action of secretly tracking (or logging) keystrokes on a keyboard, without the person using the keyboard knowing that its actions are being monitored. The collected information is stored and then sent to the attacker who can then use the data to figure out passwords, usernames and payment details, for example. There are various methods used to perform keylogging, ranging from hardware and software-based approaches to the more sophisticated electromagnetic and acoustic analysis \cite{MullinsMT}. Key loggers can be inserted into a system through phishing, social engineering or malicious downloads.
	
	There are various methods used to perform keylogging, ranging from hardware and software based approaches to electromagnetic and acoustic analysis.
	
	To this extent keyloggers can be considered as a sub-category of spyware.
	
	Keylogging also has legitimate uses, in fact it is often used by law enforcement, parents, and jealous or suspicious spouses. The most common use, however, is in the workplace, where employers monitor employee use of company computers.
	
	\item \textbf{\textit{RAM Scraper}}. \textit{RAM scraper} malware, also known as \textit{Point-of-Sale (POS)} malware, targets POS systems like cash registers or vendor portals, harvesting data temporarily stored in RAM (Random Access Memory). Doing so the attacker can access unencrypted credit card numbers \cite{IngallsTOM}.
	
	\item \textbf{\textit{Ransomware}}. \textit{Ransomware}, also known as "encryption" or "crypto" Trojan, is a malicious program that, after having infected a host or network, holds the system captive and requests a ransom from the host/network users. In particular it encrypts data on the infected system (or anyway locks down the system so that the users have no access) and only unblocks it when the correct password - decryption key - is entered. The latter is not given to the victims until after they have paid the ransom to the attacker. Messages informing the system user of the attack and demanding a ransom are usually displayed. Without the correct decryption key, it's mathematically impossible for victims to decrypt and regain access to their files.
	
	Digital currencies such as Bitcoin and Ether are the most common means of payment, making it difficult to track the cybercriminals. Moreover, paying the ransom does not guarantee the user to receive the necessary decryption key or that the one provided is correct and functions properly. Additionally, some forms of ransomware threaten victims to publicize sensitive information within the encrypted data.
	
	Ransomware is one of the most profitable, and therefore one of the most popular, and dangerous kinds of malware programs of the past few years.
	
	The "Five Uneasy E's" of ransomware, according to Tim Femister \cite{FemisterEHL} - vice president of digital infrastructure at ConvergeOne - are:
	
	\begin{itemize}
		\item \textbf{Exfiltrate}: Capture and send data to a remote attacker server for later leverage.
		\item \textbf{Eliminate}: Identify and delete enterprise backups to improve odds of payment.
		\item \textbf{Encrypt}: Use leading encryption protocols to fully encrypt data.
		\item \textbf{Expose}: Provide proof of data and threaten public exposure and a data auction if payment is not made.
		\item \textbf{Extort}: Demand an exorbitant payment paid via cryptocurrency.
	\end{itemize}

	\item \textbf{\textit{Rogue Security Software}}. \textit{Rogue Security Software} can be considered as a from of scareware. This type of malware program presents itself as a security tool to remove risks from the user's system. In reality, this fake security software installs more malware onto their system \cite{IngallsTOM}.
	
	\item \textbf{\textit{Rootkit}}. A \textit{rootkit} is generally thought as a type of malicious software, or a collection of software tools, designed to remotely access or control a computer without being detected by users or security programs. An attacker who has installed a rootkit on a system is able to remotely execute files, log user activities, access/steal information, modify system configurations, alter software (including security software), install hidden malware, mount attacks on other systems or control the computer as part of a botnet. Since a rootkit operates stealthily and continually hides its presence, its prevention, detection and removal can be difficult; in fact, typical security product are often not effective in detecting rootkits. Rootkit detection therefore often relies on manual methods such as monitoring the computer's behaviour for irregular activity, scanning system file signatures, and analysing storage dumps \cite{DuPaulCMT}.
	
	More recently, the term "rootkit" has also often been used to refer to concealment routines in a malicious program. These routines are highly advanced and complex and are written to hide malware within legitimate processes on the infected computer. In fact, once a malicious program has been installed on a system, it is essential that it remains hidden, to avoid detection and disinfection. The same is true when a human attacker directly breaks into a computer. Techniques known as rootkits allow for this concealment by modifying the host's operating system so that malware is hidden from the user. They can prevent a malicious process from being visible in the system's process list or prevent its files from being read \cite{MullinsMT}.
	
	Traditionally, rootkits can install themselves in kernel level (ring 0), although some sources state that they can install themselves all the way up to user level (ring 3). This means that they can get as much (or as little) access as necessary.
	
	There are different types of rootkits, which are typically categorized by the reach of the system they affect: \cite{IngallsTOM}
	
	\begin{itemize}
		\item \textbf{\textit{User-level/application level rootkits}} - User-mode rootkits run in Ring 3, along with other applications as user. They can alter security settings, allowing the attacker to replace executables and system libraries.
		
		\item \textbf{\textit{Kernel-level rootkits}} - Kernel-mode rootkits run in ring 0, the highest operating system privileges (Ring 0). They manage to do so by modifying the core functionality of the operating system - the kernel. They usually add code or replace portions of the core operating system, including both the kernel and associated device drivers.
		
		\item \textbf{\textit{Bootkit rootkits}} - A Bootkit rootkit is a type of kernel-mode rootkit which infects startup code like the Master Boot Record (MBR), Volume Boot Record (VBR), or boot sector, subverting the kernel upon computer start up.
		
		\item \textbf{\textit{Virtualization rootkits}} - This type of rootkit, also called \textit{Hypervisor rootkit}, runs in Ring -1 (before the kernel) and hosts the target operating system as a virtual machine. It manages to do so by exploiting hardware virtualization features. This in turn enables the rootkit to intercept hardware calls made by the original OS.
		
		\item \textbf{\textit{Hardware/firmware rootkits}} - A firmware rootkit uses device or platform firmware to create a persistent malware image in hardware. The rootkit hides in firmware, because the latter is not usually inspected for code integrity.
	\end{itemize}

	\item \textbf{\textit{Scareware}}. Scareware is a generic term for malware that uses social engineering to frighten and manipulate a user, inducing him into thinking their system is vulnerable or has been attacked.
	However, in reality no danger has actually been detected: it is a scam.	The attack succeeds when the user purchases unwanted - and potentially dangerous - software in an attempt to eliminate the "threat". Generally, the suggested software is additional malware or allegedly protective software with no value whatsoever \cite{MyraSecurityWIM}.
	
	Both Rogue Security Software and Ransomware can be considered as scareware, together with other scam software.

	Some versions of scareware act as a sort of shadow version of ransomware; they claim to have taken control of the victim's system and demand a ransom. However they are actually just using tricks - such as browser redirect loops - to fool the victim into thinking they have done more damage than they really have \cite{FruhlingerME}.
	
	\item \textbf{\textit{Spyware}}. 
	
	\textit{Spyware}, another name for \textit{privacy-invasive software}, is a type of malicious software that uses functions in the infected host's operating system with the aim of spying on the user activity. Specifically it can collect various types of personal information about users, such as Internet browsing habits, credit card details and passwords, without their knowledge. The information gathered is then sent back to the responsible cybercriminal(s). The presence of spyware is typically hidden from the user, and can be difficult to detect.
		
	However, the functions of spyware often go far beyond simple activity monitoring and information gathering. In fact, they may also interfere with the user's control of the computer in other ways, such as installing additional software and redirecting web browser activity. Spyware is known to change computer settings, often resulting in slow connection speeds, different home pages, and/or loss of Internet connection or functionality of other programs. They spread by attaching themselves to legitimate software, Trojan horses, or even by exploiting known software vulnerabilities \cite{MullinsMT}.
	
	Law enforcement, government agencies and information security organizations often use spyware to monitor communications in a sensitive environment or during an investigation. Spyware is however also available to private consumers, allowing them to spy on their employees, spouse and children \cite{McAfeeWIM}.
\end{itemize}

\subsubsection{Other cyber-threats}
Other cyber threats which are not strictly malware are, for example:

\begin{itemize}
	\item \textbf{\textit{Software Bug}}.
	A software bug is an error, or flaw, in a computer program code or system that causes it to produce an incorrect or unexpected result, or to behave in unintended ways. Usually, most of these defects arise from human errors made in the program's source code. Some bugs may be caused by compilers or operating systems component used by the program.
	
	Minor bugs only slightly affect the behaviour of a program. Therefore it can be a long time before they are discovered. On the other hand, more significant bugs can cause crashes or freezes. It is safe to say that almost all software has bugs and most bugs go unnoticed or have a slight impact on the user.
	
	However, other bugs qualify as security bugs. These are the most serious type of bugs since they can allow attackers to bypass access controls such as user authentication, override access privileges, or steal data.
	
	The frequency of bugs can be reduced through developer training, quality control, and code analysis tools \cite{DuPaulCMT}.
	
	\item \textbf{\textit{Malvertising}}. Malvertising is the use of legitimate ads or ad networks to covertly deliver malware to unsuspecting users' computers.
	
	For example, a cybercriminal might pay to place an ad on a legitimate website. When a user clicks on the ad, code in the ad either redirects them to a malicious website or installs malware on their computer.
	
	In some cases, the malware embedded in an ad might execute automatically without any action from the user, a technique referred to as a "drive-by-download".
	
	\item \textbf{\textit{Phishing}}. Phishing is a type of social engineering attack commonly used to perform cyber attacks. Particularly in a phishing attack, the attacker attempts, through email messages, to trick users into divulging passwords (or anyway personal and financial information), downloading a malicious attachment or visiting a website that installs malware on their systems.
		
	Some phishing emails are highly sophisticated and can deceive even experienced users, especially if the attacker has successfully compromised a known contact's email account and uses it to spread phishing attacks or malware such as worms. Others are less sophisticated and simply spam as many emails as possible with messages such as "Check your bank account details" \cite{ComtactWDYM}.
		
	There are different types of Phishing. Here are mentioned some of them: \cite{IngallsTOM}
	
	\begin{itemize}
		\item \textit{Deceptive Phishing} - The most common type. It uses an email headline with a sense of urgency from a know contact. This attack blends legitimate links with malicious code, modifies brand logos, and evades detection with minimal content.
		
		\item \textit{Spear Phishing} - Spear phishing targets specific users or organizations by researching the victim to maximise trick potential. For example the attacker may explore social media, record out-of-office notifications, compromise API tokens etc. in order to better fool the target user.
		
		\item \textit{Whaling} - Whaling is similar to spear phishing, but even more targeted. In fact, it targets chief officers of organizations using various social engineering tricks such as impersonating employees or co-workers and using phone calls - to name a few - to give a sense of legitimacy to malicious emails.
		
		\item \textit{Vishing} - Vishing targets phone users. It uses the Voice over Internet Protocol (VoIP), technical jargon, and ID spoofing to trick a caller into revealing sensitive information.
		
		\item \textit{Smishing} - Smishing also targets phone users. It uses, however malicious text messages (SMS).
		
		\item \textit{Pharming} - Pharming leverages cache poisoning against the DNS with the objective of redirecting users to fake websites.
	\end{itemize}

	\item \textbf{\textit{Spam}}.
	
	In cybersecurity, unsolicited emails are generally referred to as \textit{spam}. Typically, spam includes emails carrying unsolicited advertisements, fraud attempts, links to malicious websites or malicious attachments. Most spam emails contain one or more of the following: \cite{IngallsTOM}
	
	\begin{itemize}
		\item Poor spelling and grammar
		\item Unusual sender address
		\item Unrealistic claims
		\item Suspicious links
	\end{itemize}

	Spam might be one of the most universally understood forms of malicious attacks. As billions of users enable email for their everyday lives, it makes sense that malicious actors try to sneak into their inbox. Some of the most common types of spam emails include fake responses, PayPal, returned mail, and social media. All of which are disguised as legitimate but contain malware.
\end{itemize}

\subsubsection{General considerations on malware types}
Malware samples are usually categorised both by a means of infection and a behavioural category: for instance, WannaCry is a ransomware worm.\\
Moreover, a particular piece of malware may have various forms with different attack vectors: e.g., the banking malware called \textit{Emotet} has been spotted in the wild as both a trojan and a worm \cite{FruhlingerME}.\\
Finally, many instances of malware fit into multiple categories: for example Stuxnet is both a worm, a virus and a rootkit.

Furthermore, in recent years, targeted attacks on mobile devices have also become increasingly popular.
In fact, among the huge amount of available apps, an increasing quantity is not desirable; the problem is even worse when considering third-party app stores. Even when app store providers impose filters and manual checks to prevent the malicious apps from being available, some inevitably slip through.
These mobile malware threats are as various as those targeting desktops and include Trojans, Ransomware, Advertising click fraud and more. They are mostly distributed through phishing and malicious downloads and are a particular problem for jail-broken phones, which tend to lack the default protections that were part of those devices' original operating systems.

\subsection{Malware History}
Malware history began in the 1960s. Then, hackers used to design computer viruses mainly for fun, as an exciting prank/experiment; their creations would generally display harmless messages and then spread to other computers \cite{ReganWIM}.
There are numerous examples of malware created at that time within a laboratory setting: for example the \textbf{\textit{Darwing game}} in 1962, \textbf{\textit{Creeper}} in 1971, \textbf{\textit{Rabbit Virus}} in 1974 and \textbf{\textit{Pervading Animal}} in 1975.

In particular, the malware called \textbf{\textit{Creeper}} was designed to infect mainframes on ARPANET. The program did not alter the machines' functions, nor it stole or deleted data. It only displayed the message "I'm the creeper: Catch me if you can" while illegitimately spreading from one mainframe to another. This malware was later upgraded with the ability to self-replicate and became the first known computer worm \cite{LutkevichM}.

In the early 1980s, the concept of malware caught on in the technology industry, and numerous examples of viruses and worms appeared both on Apple and IBM personal computers. With the introduction of the World Wide Web and the commercial internet in the 1990s it eventually became widely popularized, so much that Yisreal Radai coined the term \textbf{malware} in 1990.

The previously mentioned 1960s and 1970s malware were all kept within a laboratory environment and never managed to escape to the wild. \textbf{\textit{Elk Cloner}} (1981) was the first known virus to have been able to escape its creation environment. Then, following the success of that prank gone wild, the first Microsoft PC virus, called \textbf{\textit{Brain}}, was created in 1986. Again, like \textit{Elk Cloner}, Brain was mostly annoying rather than harmful, but it was also the first known virus capable of concealing its presence on the disk thus evading detection. In 1988 the first worm, called \textbf{\textit{Morris}} worm, an experimental, self-propagating, self-replicating program was released on the internet \cite{NamanyaTWM}. In 1988 made its appearance also the first example of intentionally harmful virus, the \textbf{\textit{Vienna}} virus, which encrypted data and destroyed files. This led to the creation of the first antivirus tool ever \cite{ReganWIM}.

In the following decades malware has evolved both regarding its complexity and malware sample numbers.

The growth in malware complexity can be divided 5 different malware generations \cite{NamanyaTWM}:

\begin{itemize}
	\item \textit{First generation}: (DOS Viruses) malware mainly replicate with the assistance of human activity
	\item \textit{Second generation}: malware self-replicate without help and share the functionality characteristics of the first generation. They propagate through files and media.
	\item \textit{Third generation}: malware utilise the capabilities of the internet in their propagation vectors leading to big impact viruses.
	\item \textit{Fourth generation}: malware are more organization-specific and use multiple vectors to attack mainly anti-virus software or systems due to the commercialisation of malware.
	\item \textit{Fifth generation}: malware is used in cyberwarfare and the now popular malware as-a-service makes its appearance.
\end{itemize}

Each jump in generation is linked to an increase in malware complexity and more propagation vectors being available. Newer generations of malware always re-utilise older techniques while introducing newer ones. Finally newer generations are more and more evasive due to the commercial value in having access to exploited systems.

Here is an overview of the most famous malware or malware-related events in recent history:

\begin{itemize}
	\item \textbf{\textit{Melissa}} (1999) - Considered to be one of the first cases of social engineering in history, the Melissa mass-mailing macro virus infected thousands of computers worldwide by the end of 1999. The virus was spread via e-mail, using a malicious Word attachment named "list.doc" and as subject: "Important message from", followed by the victim's username. Once opened, the attachment would execute a macro that mass-mailed the virus to the first 50 people in the user's contact list and disabled multiple security features on Microsoft Word and Microsoft Outlook.
	
	\item \textbf{\textit{ILOVEYOU}} (2000) - \textit{ILOVEYOU}, sometimes referred to as \textit{Love Bug} or \textit{Love Letter for you}, was a computer worm that infected over ten million Windows personal computers in 2000. It spread as an email message with the subject line "ILOVEYOU" and the attachment "LOVE-LETTER-FOR-YOU.txt.vbs". Opening the attachment would activate the Visual Basic script which caused damages on the local machine, overwriting random files. Finally, the worm sent a copy of itself to all addresses in the Windows Address Book used by Microsoft Outlook.
	
	\item \textbf{\textit{SQL Slammer}} (2003) - This malware exploited a buffer overflow bug in Microsoft's SQL Server and Desktop Engine database products, causing a denial of service on some Internet hosts and dramatically slowing general Internet traffic. It spread rapidly, infecting most of its 75,000 victims within ten minutes.
	
	\item \textbf{\textit{MyDoom}} (2004) - Also known as \textit{W32.MyDoom@mm}, \textit{Novarg}, \textit{Mimail.R} and \textit{Shimgapi}, it was a computer worm which affected Microsoft Windows systems. It became known mostly because it tried hitting major technology companies, such as Google and Microsoft. It spread by email using attention-grabbing subjects, such as ?Error?, ?Test? and ?Mail Delivery System?. It became the fastest-spreading e-mail worm ever, exceeding previous records set by the Sobig worm and ILOVEYOU, a record which as of 2021 has yet to be surpassed.
	
	\item \textbf{\textit{Conficker}} (2008) - Also known as \textit{Downup}, \textit{Downadup} and \textit{Kido}, this computer worm targeted the Microsoft Windows operating system. It used flaws in Windows OS software and dictionary attacks on administrator passwords to propagate while forming a botnet. The Conficker worm infected over 15 million Windows systems including government, business and home computers in more than 190 countries. This made it the largest known computer worm infection since the 2003 \textit{Welchia}.
	
	\item \textbf{\textit{Zeus}} (2007-2009) - Also known as \textit{ZeuS}, or \textit{Zbot}, \textit{Zeus} was a Trojan horse malware that ran on Microsoft Windows. It was able to carry out many malicious and criminal tasks. However, it was most often used to steal banking information through the use of man-in-the-browser keystroke logging and form grabbing. It was also often used to install the \textit{CryptoLocker} ransomware. Zeus spread mainly through drive-by downloads and phishing mails. First identified in mid 2007, it became more widespread in 2009.
	
	\item \textbf{\textit{Stuxnet Worm}} (2010) - \textit{Stuxnet} was an extremely sophisticated worm that infected computers worldwide. It was allegedly developed by US and Israeli intelligence to hinder the Iranian nuclear program. It was introduced into the target environment (Iran's nuclear power plant) via a flash drive. Stuxnet's escape from the target environment, which was air-gapped, was not expected. Once in the wild, Stuxnet spread aggressively but mostly harmed the target Iranian nuclear facility, where it damaged uranium-enrichment centrifuges, causing little damage outside of its intended target environment \cite{BakerMCTM}.
	
	\item \textbf{\textit{CryptoLocker}} (2013) - It is considered to be one of the first widespread ransomware attacks. It targeted computers running Microsoft Windows and it propagated via infected email attachments. When activated, it encrypted certain types of files stored on local and mounted network drives using RSA public-key cryptography. The private key was stored only on the malware's control servers. A message was then displayed offering to decrypt the data if a payment (through either bitcoin or other means) was made before a certain deadline. After such deadline the private key was deleted. However, paying the ransom did not always lead to the files being decrypted.
	
	Its code now keeps getting repurposed in similar malware projects.
	
	\item \textbf{\textit{Mirai}} (2016) - \textit{Mirai} is an infamous malware which compromised vulnerable IoT (Internet of Things) devices - such as IP cameras and home routers - turning them into remotely controlled bots.
	
	The Mirai botnet, one of the biggest (and worse) botnets in existence, was first found in August 2016 and has been used in some of the largest and most disruptive distributed denial of service (DDoS) attacks, including an attack on computer security journalist Brian Krebs' web site. The source code for Mirai has been published on Hack Forums as open-source, as a result its techniques have been adapted in many other malware projects.
	
	\item \textit{\textbf{Petya} and \textbf{NotPetya}} (2016-2017) - These malware attacks spread globally, however their damages particularly targeted Ukraine, where the national bank was hit. The Petya ransomware family caused an estimated \$10 billion in damages worldwide \cite{ReganWIM}. \textit{Petya} targeted Microsoft Windows-based systems, infecting the Master Boot Record (MBR) to execute a payload that encrypted the hard drive's file system table preventing Windows from booting. It subsequently demanded a ransom in Bitcoin to the user in order to regain access to the system.
	
	Many variants of Petya were created in the subsequent months. In mid 2017, a new variant of Petya was used for a global cyber-attack, again primarily targeting Ukraine. The new variant spread using the EternalBlue exploit, the same one used by the WannaCry ransomware. This new version was called \textit{NotPetya} to distinguish it from the 2016 variants.
	
	\item \textbf{\textit{WannaCry}} (2017) - WannaCry is considered to be one of largest ransomware attacks in history. It targeted computers running Microsoft Windows by encrypting data and demanding ransom payments in Bitcoin. It mainly propagated through EternalBlue, an exploit developed by the U.S. National Security Agency (NSA) for older Windows systems. This exploit was stolen from NSA and leaked approximately one year before the attack. While Microsoft had released security patches against this exploit, some organizations had not applied them, or were using older Windows systems when the attack occurred.
	
	The attack was stopped within a few days of its discovery thanks to emergency patches released by Microsoft and the discovery of a kill switch. Before being stopped, WannaCry was spread infecting systems at a terrifying rate of 10,000 PCs per hour \cite{ReganWIM}. In the end, the attack was estimated to have affected more than 200,000 computers across 150 countries, with total damages ranging from hundreds of millions to billions of dollars.
	
	\item \textbf{\textit{Emotet}} (2018) - This malware, also known as \textit{Heodo}, was first detected in 2014 as a banking trojan aimed at stealing banking credentials from infected hosts. Throughout 2016 and 2017, its creators updated and reconfigured it to work primarily as a "loader" - a type of malware that gains access to a system, and then allows its operators to download and execute additional payloads. These payloads can be any type of executable code, from Emotet's own modules to malware developed by other cybercriminals.
	
	This malware usually makes its way on target systems via a macro virus attached to an email. The infected email appears to be a legitimate reply to an earlier message sent by the victim. When on the system, it is particularly difficult to combat because it evades signature-based detection, is persistent, and includes advanced spreader modules used to propagate effectively. 
	Emotet authors have often used the malware to create a botnet of infected computers to which they sell access in Malware-as-a-Service to other cybercriminals, such as the Ryuk gang.
	
	In 2020, Emotet spread again globally, infecting its victims with TrickBot and Qbot, which are used to steal banking credentials and spread inside networks. In January 2021, Europol and Eurojust coordinated international actions allowed investigators to take control of and disrupt the Emotet infrastructure.
	
	\item \textbf{\textit{COVID-19 related attacks}} (2020) - In 2020, many cybercriminals shamelessly took advantage of the people's fear of coronavirus during the COVID-19 pandemic through COVID-19 related phishing scams. Using fake communications, for example spoofing the World Health Organization, attackers deployed malware and got access to targets' sensitive information among other nefarious actions \cite{ReganWIM}.
	
	Another COVID-19 attack was that of a malicious Android app called \textit{CovidLock}, which claimed to be a real-time coronavirus outbreak tracker but instead was a ransomware that attempted to trick the user into providing administrative access on their device and then locked it requesting a ransom.
\end{itemize}

\section{Detection evasion}
From the creation of the first malware in 1970 \cite{Szor_P}, there has been a strong competition between attackers and defenders. To defend from malware attacks, anti-malware groups have been developing increasingly complex (and clever) new techniques. On the other hand, malware developers have conceived and adopted new tactics/methods to avoid the malware detectors.

The first type of anti-malware tools were mostly based on the assumption that malware structures do not change appreciably during time. In fact, initially, the malware machine code was completely unprotected. This allowed analysts to exploit opcode sequences to recognise specific malware families. 
Recently, however, a big advancement led to the so-called "second generation" malware \cite{Sharma_2014} which, to evade such opcode signatures, employs several obfuscation techniques and can create variants of itself. This posed a challenge to anti-malware developers.

The first time a malware has been recognised to exhibit detection avoidance behaviour was in 1986 with the \textit{Brain} virus \cite{SkoudisFMC}. In fact, such malware managed to conceal the infected disk section whenever the user attempted to read it, forcing the computer to display clean data instead of the infected part. From that moment on, the ever increasing popularity of detection evasion techniques among malware writers has shown that malware survival has become the number one priority: the longer the malware remains undetected, the more harm it can do and the more profitable it is to its writer \cite{NamanyaTWM}.

\subsection{Reverse-Engineering}
\textit{Reverse engineering}, in broad terms, indicates the process of extracting knowledge, ideas, design philosophy etc. from anything man-made \cite{EilamRSRE}.

Software reverse engineering is, of course, the application of reversing methodologies and techniques to extract knowledge from a software product to better understand its inner workings.

Reversing is used extensively by both malicious actors and investigators but with opposing purposes. Malware developers often use it to discover vulnerabilities in systems or programs, while analysts and antivirus software developers use it to analyse malicious programs to understand how they work, what damages they can cause, how they infect the system and reproduce, how they can be removed, detected and avoided.

\subsection{Malware analysis}
Malware analysis is the process of extracting as much information as possible from malicious samples discovered in the wild, which usually are in the form of machine code executables (compiled executables), in order to determine their purpose and functionality (and threats associated). This process allows security teams to develop effective detection techniques against the analysed malicious code, contain its damage, reverse its effects on the system, develop removal tools that can delete it from infected machines (to cleanly remove a piece of malware from an infected machine it is usually not enough to delete the binary itself) and design methods to guard systems against future infections \cite{BayerDAMC}.

Initially malware analysts/researchers had to manually analyse each malware sample. This process is however complex, requires high expertise, and is time-intensive. Moreover, the number of malware samples that need to be analysed on a daily basis is nowadays of the order of hundreds. This implies that the analysis of malware samples can no longer be done exclusively manually. Several analysis tools have been developed in recent years to facilitate analysts in analysing malware samples.

Traditionally, there are two main types of analysis: \textit{static} and \textit{dynamic}. Moreover, these two types can be, and frequently are, combined together (\textit{hybrid} analysis) in various stages of malware analysis to optimize results \cite{NamanyaTWM}.

\subsubsection{Static analysis}
\textit{Static analysis} consists of examining an executable file's code without actually executing it. Static analysis techniques usually extract peculiar features from malicious samples in order to be able to recognise them and distinguish them from benign ones. The features usually extracted are, for example, string signatures, byte-sequence n-grams, library or API calls, opcode frequency distributions, peculiar attributes found in the executable header etc. However, this approach, being based on signatures/features extracted from already analysed samples, is not much effective on zero-day and evolutionary malware.

A malware analyst performing manual static analysis usually disassembles the binary first, meaning that he 'translates' the program's machine code instructions back into assembly language ones generating a more human-interpretable code listing. On the latter, control flow, data flow analysis, and many others static techniques can be employed to try understating the program functionality and inner workings, among other useful information \cite{BayerDAMC}.

Static analysis advantages are, among others, that it takes into account the entire program code and it is also usually faster (and safer) than the dynamic one. However, a general disadvantage of static analysis is that many times the information collected during this type of analysis is very simple and not always sufficient for a conclusive decision on the malicious intent of a file. It is, however, good practice to start the analysis of a suspicious executable file extracting as much information as possible through various static techniques before passing to the dynamic counterpart. The information statically extracted may in fact provide useful knowledge to better apply dynamic techniques and enhance the final results.

Additionally, another common problem to deal with when using static analysis is that, since malicious code is written directly by the adversary, it can be purposefully designed to be hard to analyse statically. For example, analysis evasion techniques like packing, encryption and obfuscation can be exploited by malware authors to hinder both disassembly and code analysis steps typical of static analysis approaches, ultimately leading to incorrect or useless information \cite{NamanyaTWM}.

\subsubsection{Dynamic analysis}
Contrary to static analysis, \textit{dynamic techniques} analyse the program's code while or after execution in a controlled environment. These techniques, while being non-exhaustive, they have the significant advantage that they analyse only those instructions that are actually executed by the running process. This implies that dynamic analysis is less susceptible to anti-analysis attempts like code obfuscation or anti-disassembly \cite{BayerDAMC}. Moreover, dynamic analysis is also more effective in terms of malicious behaviour detection, since it doesn't look at the disassembled code but, through the use of monitoring tools, it tracks the operations that the code performs on the file system, registry, network etc. It is however, computationally more expensive and time consuming.

Basic dynamic analysis consists of observing the sample under analysis interacting with the system. For example, this can be done taking a snapshot of the original system state, introducing the malware into the system, executing it and finally comparing the new system state with the original one. The changes detected can then be used for infection removal on infected systems and/or for modelling effective signatures/features.

Advanced dynamic analysis, on the other hand, consists of directly examining the executed malware internal state while it is being run. This is done typically by monitoring the APIs and OS function calls invoked, the files created and/or deleted, the registry changes and the data processed by the program under analysis during its interaction with the system. The information extracted in this way can be used to understand the malware behaviour and functionality \cite{NamanyaTWM}.

When using dynamic techniques, however, malware analysts don't simply run malware executables on their own computer, which most probably is even connected to Internet, as they could easily escape the analysis environment and infect other hosts/networks. It is, in fact, advised to deploy dynamic techniques on "safe" and controlled (isolated) environments such as dedicated stand-alone (and isolated) hosts, virtual machines or emulators.

The use of clean dedicated hosts, reinstalled after each dynamic analysis run, is however not the most efficient solution due to the environment re-installation process overheads. On the other hand, using virtual machines (for example VMware) to perform dynamic analysis is more efficient. In fact, in this case, since the malware only affects the virtual machine environment, it is enough, after a dynamic analysis run, to simply discard the infected hard disk image and replace it with a clean one. Unfortunately, a significant drawback is that the malware being analysed may determine it is running in a virtualized environment and, as a result, modify its behaviour. To counter this last problem one could make use of emulators, which are theoretically undetectable by analysed malware. These tools, however, run the code under analysis significantly slower and are therefore sometimes detectable using specially crafted time-related code.

\subsubsection{Hybrid analysis}
Hybrid Analysis is the combination of static and dynamic analysis. It is a technique that integrates run-time information extracted through dynamic analysis with information extracted  through static analysis in order to have a complete view of the malware's behaviour while avoiding the problems posed by anti-analysis techniques as much as possible.

\subsection{Anti-reversing}
\textit{Anti-reversing} techniques are techniques originally meant to make the reverse engineering process difficult for a hacker or any malicious user. The main objective of various anti-reverse engineering techniques is simply to complicate the process of reversing as much as possible. For example an attacker could use the disassembly of a binary in order to get an insight of the logic of the code as well as getting hidden information.

Recently anti-reversing techniques are, however, extensively used also by malware authors in order to make their creations difficult to analyse in an attempt to postpone detection as much as possible.

There exist several anti-reversing approaches, each with its own advantages and disadvantages. However it is common practice to use a combination of more than one of them. In the next sections some of the more common anti-reversing techniques are discussed.

\subsection{Anti-disassembly}
\textit{Anti-disassembly} techniques use specially crafted code and/or data in a program to cause disassembly analysis tools to generate an incorrect program listing \cite{SikorskiPMA}. The attackers' usage of these techniques thus implies a time-consuming analysis for malware analysts, ultimately preventing the retrieval of the source code in a reasonable time.

Any executable code can be reverse engineered, but by armouring their code with anti-disassembly and anti-debugging techniques, attackers increase the skill level required by analysts.
Furthermore, anti-disassembly techniques may also inhibit various automated analysis tools and heuristic-based engines which take advantage of disassembly analysis to identify or classify malware.

These techniques exploit the inherent weaknesses present in disassembler algorithms. Moreover, disassemblers, in order to work properly, make certain assumptions on the code being analysed. However, when these assumptions are not met, there is an opportunity for malware authors to deceive the analyst.

For example, while disassembling a program, sequences of executable code can have multiple disassembly representations, some of which may be invalid and obscure the real purpose of the program.
Thus, the malware authors, in order to add anti-disassembly functionality to their creations, can produce sequences of code that deceive the disassembler into outputting a list of instructions that differs from those that would be executed \cite{SikorskiPMA}.

There are two types of disassembler algorithms: linear and flow-oriented (recursive).
The linear one is easier to implement, but it is also more simplistic and error-prone.

\subsubsection{Linear Disassemblers}
The \textit{linear} disassembly strategy is based upon the basic assumption that the program's instructions are organized one after the other, linearly. In fact, this type of disassemblers iterates over a block of code, disassembling one instruction at a time, sequentially, without deviating. More specifically, the tool uses the size of the currently disassembled instruction to figure out what bytes to disassemble next, without accounting for control-flow instructions \cite{SikorskiPMA}.

Linear disassemblers are easy to implement and work reasonably well when working with small sections of code. They introduce, however, occasional errors even with non-malicious binaries. The main drawback of this technique is that it blindly disassembles code until the end of the section, assuming the data is nothing but instructions packed together, without being able to distinguish between code, data and pointers.

In a PE-formatted executable file, for example, the executable code is typically contained inside a single ".text" section. However, for almost all binaries, this code section contains also data, such as pointer values. These pointers will be blindly disassembled and interpreted by the linear disassembler as instructions.

Malware authors can exploit this weakness of linear-disassembly algorithms implanting data bytes that form the opcodes of multi-byte instructions in the code section.

\subsubsection{Flow-Oriented Disassemblers}
The \textit{flow-oriented} (or \textit{recursive}) disassembly strategy is more advanced than the previous one and is, in fact, the one used by most commercial disassemblers like \textit{IDA Pro} \cite{SikorskiPMA}.

Differently form the linear strategy, the flow oriented one examines each instruction, builds a list of locations to disassemble (the ones reached by code) and keeps track of the code flow.

This implies that, if disassembling a code section we find a JMP instruction, this type of disassembler will not blindly parse the bytes immediately following the JMP instruction?s ones, but it will disassemble the bytes at the jump destination address.

This behaviour is more resilient and generally provides better results, but also implies a greater complexity.

In fact, while a linear disassembler has no choices to make about which instructions to disassemble at any given time, flow-oriented disassemblers have to make choices and assumptions, in particular when dealing with conditional branches and call instructions.\\
Particularly, in the case of conditional branches, the disassembler needs to follow both the false branch (most flow-oriented disassemblers will process the false branch of any conditional jump first) and the true one. In typical compiler-generated code there would be no difference in output if the disassembler processes first one branch or the other. However, in handwritten assembly code and anti-disassembly code, taking first one branch or the other can often produce different disassembly for the same block of code, leading to problems in analysis.

\subsubsection{Anti-Disassembly Techniques}

\paragraph{Jump Instructions with the Same Target}
One of the most used anti-disassembly techniques consists of two consecutive conditional \textit{jump} instructions both pointing to the same target \cite{SikorskiPMA}.

Here is an example:
\begin{lstlisting}[caption={Jump Instructions with the Same Target}, label=JumpSameTarget, language={[x86masm]Assembler}, style=mystyle]
	74 03		jz 	loc
	75 01		jnz	loc
	
	loc:
\end{lstlisting}

In this case, the conditional jump '\textbf{jz loc}' is immediately followed by a jump to the same target but with opposite condition: '\textbf{jnz loc}'. This implies that the location \textbf{loc} will always be jumped to.
Consequently, the combination of \textbf{jz} with \textbf{jnz} acts, in this case, like an unconditional \textbf{jmp} instruction. A disassembler, however, since it disassembles just one instruction at a time, won't recognize this combination as being an unconditional branch. During the disassembly process, in fact, if a \textbf{jnz} instruction is encountered, the disassembler will take the false branch of the instruction and will continue disassembling, even though this branch will never be executed in practice.

\paragraph{Jump Instructions with a Constant Condition}
Another common anti-disassembly technique is composed of a single conditional \textit{jump} instruction with an always true (or false) condition \cite{SikorskiPMA}.

Example:
\begin{lstlisting}[caption={Jump Instructions with a Constant Condition example}, label=JumpConstant, language={[x86masm]Assembler}, style=mystyle]
	33 C0		xor	eax, eax
	74 01		jz 	loc
	
	loc:
\end{lstlisting}

The first instruction in the example code, \textbf{xor eax, eax}, sets the \textbf{EAX} register to zero and, consequently, it sets the zero flag. The next instruction, \textbf{jz} (jump if zero flag is set), appears to be a conditional jump but in reality is not conditional at all. In fact the the zero flag will always be set at this point in the program execution. The disassembler, however, will process the false branch first, even if in reality it would never trigger.

\paragraph{Impossible Disassembly}
The simple anti-disassembly techniques mentioned above are frequently coupled with the use of a, so called, \textit{rogue byte}. A \textit{rogue byte} is a data byte strategically placed after a conditional \textit{jump} instruction in order to trick the disassembler. The byte inserted usually is the opcode for a multi-byte instruction, therefore disassembling it prevents the real following instruction from being properly disassembled. This byte is called \textit{rogue byte} because it is not part of the program logic flow and it is inserted in the code with the only purpose of fooling the disassembler \cite{SikorskiPMA}.

In all these cases, however, a reverse engineer is able to properly disassemble the code with the use of interactive disassemblers like IDA Pro, ignoring the \textit{rogue bytes}.

However, there are some conditions in which no traditional assembly listing can accurately represent the instructions that are executed. Exploiting these conditions we obtain what are called \textit{impossible disassembly} techniques. The code produced using these techniques can however be disassembled, but only using a vastly different representation of the code than what is provided by currently available disassemblers.

The core idea behind these techniques is to make the \textit{rogue byte} part of a legitimate instruction that is executed at runtime. This way the \textit{rogue byte} becomes not ignorable during disassembly. In this scenario any given byte may be a part of multiple instructions that are executed. This is done using \textit{jump} instructions. The processor, while running the code, will interpret and execute the bytes following the logical flow of the program, so there is no limitation on the number of instructions the same byte can be part of; a disassembler, however, has such limitations since it will usually represent a single byte as being part of a single instruction.

Example:
\begin{lstlisting}[caption={Impossible Disassembly example}, label=ImpossibleDisassembly, language={[x86masm]Assembler}, style=mystyle]
	EB 
				JMP -1
	FF
				INC EAX
	C0
	48		DEC EAX
\end{lstlisting}

In this simple example the first instruction is a 2-byte \textit{jmp -1} instruction (\textbf{EB FF}). Its target is the its own second byte. At run time this causes no errors because the \textbf{FF} byte is the first byte of the next instruction \textit{inc eax} (\textbf{FF C0}).

However, when disassembling, if the disassembler interprets the \textbf{FF} byte as part of the \textit{jmp} instruction, it won't be able to interpret it also as the beginning of the \textit{inc eax} instruction. While the \textbf{FF} byte is in reality part of both instructions that actually execute, the disassembler is not able to recognise this.

The 4-byte example code increments the \textbf{EAX} register, and then decrements it, therefore it is essentially a complex \textbf{NOP} sequence. Being a simple, and small, sequence it could be inserted at any location in a program code in order to fool disassemblers. However this sequence it is also easily recognisable by reverse engineers and substituted with \textbf{NOP} instructions using IDA Pro or other instruments and/or scripts. Another alternative is to interpret this sequence as data bytes forcing the disassembler to skip it.

However this was only a simple example sequence. More complex and ingenious sequences can be made to fool disassemblers while being harder to detect.

\subsubsection{Obscuring Flow Control}
Control-flow analysis (CFA) is a static-code-analysis technique for determining the control flow of a program. Modern disassemblers like IDA Pro are able to correlate function calls and extract high-level information about the program knowing how functions are related to each other \cite{SikorskiPMA}.

Control-flow analysis can however be easily defeated by malware authors.

\paragraph{The Function Pointer Problem}
Function pointers are a common programming idiom present in programming languages such as \textbf{C}, while being extensively used in the background in object oriented languages like \textbf{C++} and \textbf{Java} \cite{SikorskiPMA}.

As opposed to referencing a data value, a function pointer points to executable code within memory. Dereferencing the function pointer yields the referenced function, which can be invoked and passed arguments to as in a normal function call. Since, doing so, the function is being invoked indirectly through a variable instead of directly through a fixed identifier or address, such invocation is also known as an "indirect" call. In assembly code this corresponds to a \textit{call} instruction with a function pointer as argument.

Function pointers, however, greatly reduce the information that can be automatically extracted by the disassembler about the program control flow. Moreover, if function pointers are used in specially crafted, or non-standard code, the resulting code can be difficult to reverse-engineer without the use of dynamic analysis techniques.

As a result, function pointers, in combination with other anti-disassembly techniques, can greatly increase the complexity and difficulty of reverse-engineering.

\paragraph{Return Pointer Abuse}
Among the instructions capable of transferring control within a program we already mentioned the \textit{call} and \textit{jmp} instructions, however there are more \cite{SikorskiPMA}.

The counterpart to the \textit{call} instruction is \textit{retn}. When a call instruction is reached during program execution, a return pointer is pushed on the stack, before jumping to the call instruction target. This return pointer in the stack will point to the address of the instruction immediately following the end of the \textit{call} instruction itself.
Therefore a \textit{call} instruction can be seen as the combination of a \textit{jmp} and \textit{push}; a \textit{retn} instruction, on the other hand, is the combination of \textit{pop} and \textit{jmp}.

The \textit{retn} instruction pops the last value pushed to the stack and jumps to it; it is therefore typically used to return from a function call, but it could also be used for other purposes. When used for such other reasons the disassembler is generally fooled, because it still will interpret it as a return from a function call. Therefore it won't show any code cross-reference to the target being jumped to. As added benefit the disassembler will also prematurely terminate the function being analysed.

\paragraph{Misusing Structured Exception Handlers}\label{MisusingStructuredExceptionHandlers}
Another powerful anti-disassembly technique exploits the Structured Exception Handling (\textbf{SEH}) mechanism. Performing program flow control using this mechanism is able to fool both disassemblers and debuggers \cite{SikorskiPMA}.

\textbf{SEH} provides programs a way to handle error conditions intelligently. \textit{C++} and other programming languages heavily rely on exception handling (and therefore on \textbf{SEH}) when compiled for x86 systems.

Exceptions can be triggered for numerous reasons: for example when dividing by zero or accessing an invalid memory region. Moreover, software exception can also be raised by the code itself by calling the \textit{RaiseException} function.

When an exception is raised it makes its way through the \textbf{SEH} chain, which is a list of functions specifically designed to handle exception, until it is caught by one exception handler in the chain. Each function in the list can either handle the exception (a.k.a. \textit{catch} it) or pass it to the next handler in the list. \textit{Unhandled exceptions} are the ones that make their way to the last handler. The last exception handler is the code responsible for triggering the 'unhandled exception' message to the user. 

Exception handling is used in almost all programs and exceptions happen regularly in most processes (and are handled silently). A malicious actor could, however, exploit this mechanism to achieve covert flow control by adding his own specially crafted handler on top of the \textbf{SEH} chain.

This can be done at runtime simply pushing some specific values on the stack, effectively adding a new entry in the Exception handling chain. This procedure, however, is subject to the constraints imposed by the Software Data Execution Prevention (\textbf{Software DEP}), which is a security feature that prevents the addition of third-party exception handlers at runtime. However various workarounds to this protection can be used in the case of handwritten assembly code.

\subsection{Anti-debugging}
Another popular anti-analysis technique, besides anti-disassembly, is \textit{anti-debugging}. Malware authors use anti-debugging techniques to recognise when their malicious program is under the control of a debugger or to interfere with the debugger behaviour. This is done in an attempt to slow down the malware analysts who use debuggers to understand how the malware operates. A malware using these techniques usually alter its normal control flow paths or causes crashes if it detects it is running in a debugger, thus interfering with analysis \cite{SikorskiPMA}.

\subsubsection{Windows Debugger Detection}
In Windows OS various techniques can be used to detect if a process is being run in a debugger: from exploiting the Windows API itself, to manually checking memory structures for debugging artefacts \cite{SikorskiPMA}.


\paragraph{Using the Windows API}
One of the most obvious, and simple, ways to know if a debugger is attached to a process is by using Windows API functions. Inside the Windows API there are, in fact, functions that were specifically designed to detect debuggers; moreover some functions that were originally created with other purposes can also be used for debugger detection \cite{SikorskiPMA}.

Malware analysts can counter this technique by manually modifying the malware code during execution modifying the resulting flag after the call to make sure the desired path is taken, or by straight up removing/skipping the function call.

Here are some examples of common Windows API functions used for \textit{anti-debugging}:
\begin{itemize}
	\item \textbf{IsDebuggerPresent}
	This is the simplest API function that can be used for debugger detection. It determines whether the \textbf{current} process is being debugged by a user-mode debugger. It does so by getting the value of the field \textit{IsDebugged} from the Process Environment Block (\textbf{PEB}) structure. In particular this functions returns zero if the process is not running within a debugger context and a non-zero value otherwise.
	
	\item \textbf{CheckRemoteDebuggerPresent}
	This API function is similar to the previously described one (\textit{IsDebuggerPresent}). This function checks for a 'remote' debugger on the specified process. The term 'remote' in the name CheckRemoteDebuggerPresent does not imply that the debugger necessarily resides on a different machine; instead, it indicates that the debugger resides in a separate and parallel process. This function takes a process handle as argument, and will check if that process has a debugger attached. It can however be used also to check the current process by passing its handle.
	
	\item \textbf{NtQueryInformationProcess}
	This function can retrieve different kinds of information from a process. The first argument for this function is the process handle, the second one is the ProcessInformationClass parameter which specifies the information you want to get. When using the value \textit{ProcessDebugPort} for this parameter, for example, the function will return a zero if the process is not currently being debugged; a non-zero value representing the debugger port number will instead be returned otherwise.
	
	\item \textbf{OutputDebugString}
	This function, originally designed to just send a string to a debugger for display, can be used to detect the presence of a debugger. In fact, in there is no debugger attached, the function will internally set the last-error code. In a few lines of code it is thus possible to know if a debugger is present or not:
	
	\begin{lstlisting}[caption={OutputDebugString debugger detection}, label=OutputDebugString, language=C, style=mystyle]
	DWORD errorValue = 12345;
	// set custom last error code
	SetLastError(errorValue);
	
	// try outputting string on debugger;
	// if no debugger is present, it will set
	// the last-error code to a new value
	OutputDebugString("Test for Debugger");
	
	if(GetLastError() == errorValue){
		// a debugger is present
		ExitProcess();
	}
	else{
		// no debugger was detected
		RunMaliciousPayload();
	}
	\end{lstlisting}
	
\end{itemize}

\paragraph{Manually Checking Structures}
Malware authors usually don't exploit the Windows API functions for detecting the presence of a debugger, but they prefer checking the PEB structure (and others) by themselves. One of the reasons why they usually don't like using Windows API functions is that API calls can be easily hooked by a rootkit to return false information, thus thwarting this technique \cite{SikorskiPMA}.

\begin{itemize}
	\item \textbf{Checking the BeingDebugged Flag}
	The Windows PEB structure contains all user-mode parameters associated with a process, including the process's environment data such as environment variables, addresses in memory and debugger status, among other things.
	
	Malware can 'manually' check the \textit{BeingDebugged} flag within the PEB structure to understand if a debugger is attached its process. More precisely if this flag is zero it means that no debugger is attached.
	
	Example of code listing performing 'manual' \textit{BeingDebugged} check:
	\begin{lstlisting}[caption={BeingDebugged manual check}, label=BeingDebuggedManual, language={[x86masm]Assembler}, style=mystyle]
	mov		eax,	dword ptr fs:[30h]	; get PEB address
	mov		ebx,	byte ptr [eax+2]		; get BeingDebugged flag value
	test	ebx,	ebx									; test if the value is 0
	jz		NoDebuggerDetected				; if 0, no debugger was detected
	\end{lstlisting}
	
	Malware analysts can counter this technique detecting this code sequence in the code and then wither manually changing the \textit{BeingDebugged} flag to zero, or forcing the jump to be taken (or not) by manually modifying the zero flag before the jump instruction.
	
	\item \textbf{Checking the ProcessHeap Flag} 
	The \textit{ProcessHeap}, which is an undocumented location within a reserved array inside the PEB structure, contains the location of the first heap of a process allocated by the loader. This heap can be used for debugger detection since it contains some information telling if it was created within a debugger or not. In particular malware usually check the values of the fields called \textit{ForceFlags} and \textit{Flags}.
	
	To overcome this technique, malware analysts can change the \textit{ProcessHeap} flags manually or use a hide-debug plug-in for their debugger.
	
	\item \textbf{Checking NTGlobalFlag}
	Processes started within a debugger run slightly differently than others, therefore they create memory heaps differently. The information needed to determine how to create heap structures is stored at an undocumented location in the PEB. Practically, a value of \textit{0x70} at this location indicates that the process is running within a debugger.
	
	Again, in order to counter this technique, malware analysts can change the flags manually or use a hide-debug plug-in for their debugger.
	
\end{itemize}

\paragraph{Checking for System Residue}
Debugging tools typically leave traces of their presence on the system. Malicious programs can therefore be designed to search for these traces in the system in order to determine when it is being analysed. For example malware can search for references to debuggers in the registry keys \cite{SikorskiPMA}.

Moreover, malware can also be designed to search the system for files and directories commonly related to debuggers, such as debugger program executables.

Furthermore, malware can also detect debugger residues in live memory, by viewing the current process listing or, more commonly, by performing a \textit{FindWindow} in search for a debugger.

\subsubsection{Identifying Debugger Behaviour}
Debuggers are very useful to malware analysts because they can be used to set breakpoints in the code or even to single-step through a process running code to ease the reverse-engineering process. These operations, however, modify the process code and are therefore easily detectable \cite{SikorskiPMA}.

\paragraph{INT Scanning}
A common anti-debugging technique used by malware authors consists in making the process scan its own code in search for an \textbf{INT 3} (opcode \textit{0xCC}). \textbf{INT 3} is, in fact, a software interrupt used by debuggers: when setting a breakpoint the debugger replaces the target instruction in the running program with the opcode \textit{0xCC} (INT 3) which causes the process to call the debug exception handler \cite{SikorskiPMA}.

Malware analysts can counter this technique exploiting hardware breakpoints instead of software ones.

\paragraph{Performing Code Checksums}
Another anti-debugger technique consists in calculating the checksum of a section of the process' own code. This has the same net effects as scanning the code for software interrupts. However, instead of explicitly searching for a specific opcode (\textit{0xCC}) in the process code, this check performs a cyclic redundancy check (CRC) or a MD5 checksum of the malware code \cite{SikorskiPMA}.

Again this technique can be countered by using hardware breakpoints instead of software ones, or by modifying the program's control flow at runtime with a debugger.

\paragraph{Timing Checks}
One of the most widespread techniques for debugger detection is to perform \textit{timing checks}. Processes, in fact, tend to run substantially slower when executed within a debugger context. Moreover analysts usually run programs in single steps in order to better understand the code behaviour, this in turn greatly increases execution time \cite{SikorskiPMA}.

Using timing checks it is possible to detect a debugger in different ways:
\begin{enumerate}
	\item Recording 2 timestamps before and after the execution of some operations and then comparing them. If the lag is greater than a specified threshold then a debugger is probably being used.
	\item Recording 2 timestamps before and after raising an exception. If the current process is being debugged then the exception will be handled by the debugger itself more slowly than normal. Moreover, by default, debuggers ask for human intervention when an exception occurs, thus causing huge delays.
\end{enumerate}

\begin{itemize}
	\item \textbf{Using the \textit{rdtsc} Instruction}
	The most common timing check method uses the \textit{rdtsc} instruction. This instruction returns the number of ticks since the last system reboot. Malware authors thus use it as described above: rdtsc is called twice, once before and once after some other operations, and then the difference between the results is calculated. If too much time has elapsed between the two calls it means that a debugger is probably being used.
	
	\item \textbf{Using \textit{QueryPerformanceCounter} and \textit{GetTickCount}}
	These are two Windows API functions that can be used similarly to \textit{rdtsc} for debugger detection.
	
	More precisely \textit{QueryPerformanceCounter} can be called to query a high-resolution counter available to processors which stores counts of activities performed by the processor.
	
	The function \textit{GetTickCounter}, on the other hand, returns the number of milliseconds that have elapsed since the last reboot, very similarly to what the \textit{rdtsc} instruction does.
	
	Both of those functions, when used as described above, allow the malware to detect the presence of a debugger.
\end{itemize}

Anti-debugging though the use of timing checks can be discovered by malware analysts during debugging or static analysis by identifying specific sequences of instructions. Moreover, these checks usually detect debuggers only when the analyst is single-stepping though the code or setting a breakpoint between the two time related instruction calls.
This implies that, to counter this technique, malware analysts could avoid setting breakpoints and single-stepping in those regions of code, or modify the result of the timestamps comparison as needed.

\subsection{Anti-virtual machine}
Malware analysts often use virtual machines (VMs) or other isolated environments like sandboxes, to analyse malware samples. With the purpose of evading analysis and bypassing security systems malware authors often design their code to detect isolated environments. The techniques used with such purpose are called \textit{Anti-virtual machine} techniques (Anti-VM). Once a virtual machine is detected the evasion mechanism may alter the malware?s behaviour, or it may even prevent the malicious code from running altogether \cite{SikorskiPMA}.

\subsubsection{VMware Artefacts}
Virtual machines are designed to emulate real hardware functionality. To achieve that, however, some artefacts inevitably remain on the system, which can reveal that a virtual machine is indeed being used. These kind of artefacts can be specific files, processes, registry keys, services, network device adapters etc. \cite{SikorskiPMA}.

Here are some examples of anti-virtual machine techniques applied to detect VMware virtualization software:
\begin{itemize}
	\item \textbf{Checking for Processes Indicating a VM}.
	When a VMware virtual machine is running and VMware tools is installed, three VMware-related processes can be found in the system process listing: \textit{VMwareService.exe}, \textit{VMwareTray.exe} and \textit{VMwareUser.exe}. A malicious software can therefore easily detect if VMware is being run searching through the process listing for the \textit{VMware} string.
	
	\item \textbf{Checking for Existence of Files Indicating a VM}.
	The VMware default installation path usually also contains artefacts. Searching for the string \textit{VMware} in such location may reveal the use of a virtualized environment.
	
	\item \textbf{Checking for Registry Keys}.
	VMware Tools may leave some artefacts also in the registry. More specifically the presence of specific registry entries may reveal the use of VMware.
	
	\item \textbf{Checking for Known Mac Addresses}.
	In order to connect a virtual machine to a network it needs to have its own virtual network interface card (NIC). This implies that VMware needs to create a MAC address for the virtual machine, to associate to its NIC. However, depending on VMware configuration, this may lead to the network adapter being able to identify VMware usage. VMware utilises, in fact, addresses with a specific starting sequence which depends on its current version. Therefore a malicious software just needs to check the system MAC address against common VMware values.
\end{itemize}

In order to counter anti-virtual machine techniques, malware analysts need to apply a two step process: identify the check for VMware artefacts and then 'manually' patch it. For example, depending on the anti-VM technique used, they may patch the malware code while debugging to artificially make all checks pass, or modify the name of VMware processes in order to make them undetectable by the malicious software.

\subsubsection{Vulnerable Instructions}
The virtual machine monitor program, which monitors the virtual machine execution, has some security weaknesses that may allow malware to detect its usage. In particular, in order to avoid performance issues deriving from fully emulating all instructions, VMware allows certain instructions to execute without being properly virtualized. This in turn means that certain instruction sequences may return different results when running within a VMware virtualized environment than they do on native hardware. This discrepancy can be used by malware authors to detect VMware usage \cite{SikorskiPMA}.

However, those instructions previously mentioned are not typically used within a malicious program unless it is specifically performing VMware detection, because they are useless if executed in user mode. Therefore avoiding this type of anti-VM technique can be as easy as patching the malicious code to prevent it calling these instructions.

\subsection{Packers and unpacking}
Packing programs, commonly known as \textit{packers}, are software programs that take an \textit{executable file} or \textit{dynamic link library (DLL)}, compresses and/or encrypts its contents and then packs it into a new executable file \cite{SikorskiPMA}.

When packers are used on malicious programs, the malicious code appearance is changed as a consequence of the compression and/or encryption. The packed file will thus hinder basic static analysis and malware detection. Moreover, a packer specifically designed to make the file difficult to analyse may even employ anti-reverse-engineering techniques, such as anti-disassembly, anti-debugging or anti-VM on the resulting compressed version; on top of that some packers, using randomization, are also able to generate different variants of a single file every time it is packed \cite{LiPRTCMP}.

Malware authors have thus increasingly been using these tools to hide their creations from anti-malware solutions and malware analysts. In order to analyse packed malware, in fact, it must be unpacked first. Properly unpacking a packed program generally is, however, not easy.

A packed file usually contains two basic components:
\begin{itemize}
	\item A number of data blocks containing the compressed and/or encrypted original executable file.
	
	\item An unpacking stub able to dynamically recover the original executable file at runtime.
\end{itemize}

When the packed file is executed, the unpacking stub is loaded by the OS and begins unpacking the original executable code in memory. When the unpacking has been fully completed the control flow is transferred, with a \textit{jmp}, \textit{call} or the more stealthy \textit{retn} instruction (also referred to as the \textit{tail jump}), to the original file entry point (OEP). This implies that someone attempting to perform static analysis on the packed program, would actually analyse the unpacking stub and not the original code.

\subsubsection{Packer types}
Commercial and custom made packers can be divided in several levels of complexity depending on the packing techniques used and the additional features they have.
The authors of \cite{PedreroDPI} identified 6 packer main types with increasing complexity.
Packer types from 1 to 5 allow, sooner or later at runtime, to have a complete view over the original (unpacked) malicious code, meaning that the unpacker stub unpacks all the code at once. However, what makes them differ is the amount and complexity of the encryption (and obfuscation) methodologies used during packing. On the other hand, type 6 packers unpack only a slice of code at a time in memory, never revealing the whole original code altogether. This implies that malware analysts need to take several memory dumps, instead of only one, if they want to get the complete unpacked code.

Another possible classification of packers can be made based on their purposes and behaviours. Following this idea packers can be broadly classified into four categories \cite{WeiRPM}:

\begin{itemize}
	\item \textbf{Compressors} utilise compression to shrink files while exploiting few or no anti-unpacking tricks. Popular compressors include the Ultimate PE Packer (UPack), Ultimate Packer for Executables (\href{https://upx.github.io/}{UPX}), and \href{http://www.aspack.com/}{ASPack}.
	
	\item \textbf{Crypters} encrypt and obfuscate the original file contents. No compression is usually done. Malware developers widely use crypters such as \href{https://sourceforge.net/projects/yodap/}{Yoda's Crypter} and PolyCrypt PE.
	
	\item \textbf{Protectors} combine features from both compressors and crypters. Some popular commercial protectors are Armadillo and \href{https://www.oreans.com/Themida.php}{Themida}.
	
	\item \textbf{Bundlers} are used to pack a software package of multiple executable files into a single bundled executable file. These files within the package can then be unpacked and accessed without extracting them to disk. Some common PE bundlers are \href{https://www.bitsum.com/pebundle.asp}{PEBundle} and \href{https://www.molebox.com}{MoleBox}.
\end{itemize}

\subsubsection{Packers detection}
Packed executables can be detected through a heuristic approach known as \textit{Shannon Entropy Calculation}. Entropy is, generally speaking, a measure of uncertainty, disorder, in a system or program. The idea behind this approach is that compressed or encrypted executables tend to resemble random data, thus they have higher entropy than unencrypted/uncompressed programs. This approach, however, does not tell any information about the packer used to obtain the packed sample \cite{SikorskiPMA}.

One common way to tackle this problem is through packer signatures checking. Tools like \href{https://www.aldeid.com/wiki/PEiD}{PEiD} and Sigbuster use such method. These tools are, however, not always successful due to the huge number of packer variations and evolutions present in the wild, and the fact that malware authors frequently modify commercially available packers code or create their own packers so that their packed malicious programs do not match any known signature.

\subsubsection{Unpacking}
Unpacking is the process of restoring the original contents from packed executables in order to allow AV programs and security researchers to analyse the original executable code. There are three different techniques to unpack a packed executable: \textit{automated static unpacking}, \textit{automated dynamic unpacking} and \textit{manual unpacking} (\cite{SikorskiPMA}, \cite{WeiRPM}).

Automated static unpacking programs are dedicated routines designed to decompress and/or decrypt executables packed by specific packers, without actually executing the suspicious programs. This method, when it works, is the fastest and most secure method to unpack an executable. Automatic static unpackers are, however, specific to a single packer. Moreover, they are not able to unpack packed samples that were created with the intention to hinder analysis.

Automated dynamic unpackers, instead, use programs to run or emulate the packed executable allowing the unpacking stub to unpack the original executable code in memory. Once the original executable is unpacked, the in-memory program's code is written on disk, and the automated unpacker reconstructs the original import table.

Most often security researchers prefer to perform manual unpacking. The two most common approaches used  to manually unpack a program are:
\begin{itemize}
	\item Discover what packing algorithm has been used to pack a sample and then write a program/script to revert it. This process is however time consuming.
	
	\item Manually run the packed program to allow the unpacking stub to unpack the original code in memory, then dump the process on disk and finally manually modify the PE header so that the program is complete. This process is more efficient than the previous one. 
\end{itemize}

\subsection{Code Obfuscation}
Obfuscation is a technique that generally makes programs harder to understand \cite{Balakrishnan2005CodeOL}, both for humans and automatic tools. To do so, it transforms a program into a new (structurally different) and more difficult to analyse version while retaining the same functionality as the original (the new version of the program is said to be \textit{computationally equivalent} to the original one) \cite{YouMOT}.

Originally, this technology was conceived for legitimate purposes to protect the intellectual property of software developers; however it has been widely exploited by malware authors to evade detection \cite{KonstantinouMV}. Particularly, in order to elude anti-malware scanners, malware can, using obfuscation techniques, evolve their body into new generations \cite{YouMalwareOT}, which eventually can be even harder to disassemble and analyse.

Obfuscation techniques can be broadly divided into 2 main sub-categories:
\begin{itemize}
	\item \textit{Data-based} obfuscation
	\item \textit{Control-based} obfuscation
\end{itemize}

However, malware authors usually combine those 2 types of obfuscation techniques in complex and difficult ways to strengthen the resulting obfuscation \cite{DangPRE}.

\subsubsection{Data-Based Obfuscation}
Data-based obfuscation techniques focus on modifying data values and non-control computations. In the following paragraphs some common data-based obfuscation techniques will be discussed.

\paragraph{Constant Unfolding}
\textit{Constant folding} is a technique commonly used by compilers to optimize a program's code. It does so by replacing expressions with results known at compile time with the results themselves \cite{DangPRE}.

For example, a compiler usually transforms the following expression \ref{ConstantFoldingBefore}, into \ref{ConstantFoldingAfter}.
\begin{lstlisting}[caption={Before constant folding}, label=ConstantFoldingBefore, language=C, style=mystyle]
	x = 4 * 5;
\end{lstlisting}

\begin{lstlisting}[caption={After constant folding}, label=ConstantFoldingAfter, language=C, style=mystyle]
	x = 20;
\end{lstlisting}

\textit{Constant unfolding} is, instead, an obfuscation technique that performs the exact inverse operation: it replaces the constants in the program's code with some expressions having the constant as a result.

For example, the listing \ref{ConstantUnfoldingBefore}, after \textit{Constant unfolding} may become \ref{ConstantUnfoldingAfter}. The two listings are equivalent. Moreover, there is an infinite amount of listings equivalent to \ref{ConstantUnfoldingBefore} that can be generated following this principle.
\begin{lstlisting}[caption={Before constant unfolding}, label=ConstantUnfoldingBefore, language={[x86masm]Assembler}, style=mystyle]
	push	0h
\end{lstlisting}

\begin{lstlisting}[caption={After constant unfolding}, label=ConstantUnfoldingAfter, language={[x86masm]Assembler}, style=mystyle]
	push	0F9CBE47Ah
	add		dword ptr [esp],	6341B86h
\end{lstlisting}

\paragraph{Data-Encoding Schemes}
The previously described technique is, however, easily defeated by simply applying the standard compiler's constant folding optimization. This is possible because both the data encoding and decoding functions ($f(x)=x-6341B86h$ and $f_{-1}(x)=x+6341B86h$ respectively) were present in the code one after the other. The use of \textit{fully Homomorphic} mappings (operation-preserving mappings) allow us to perform some operations on the encoded data before decoding it back, thus overcoming the previous technique's flaw. \textit{Fully Homomorphic} mappings are however still not widely used because still too inefficient \cite{DangPRE}.

\paragraph{Dead Code Insertion}
\textit{Dead code elimination} is another common compiler optimization technique. Its objective is to remove program statements/expressions that nave no real effects on the program operation and final results \cite{DangPRE}.

For example, the listing \ref{DeadCodeEliminationBefore}, using \textit{dead code elimination} would become \ref{DeadCodeEliminationAfter}.
\begin{lstlisting}[caption={Before dead code elimination}, label=DeadCodeEliminationBefore, language=C, style=mystyle]
	int f(){
		int x, y;
		x = 1;		// this assignement is useless, here x is dead
		y = 2;		// y is never used, it is thus dead.
		x = 3;
		return x;	// x is live
	}
\end{lstlisting}

\begin{lstlisting}[caption={After dead code elimination}, label=DeadCodeEliminationAfter, language=C, style=mystyle]
	int f(){
		return 3;
	}
\end{lstlisting}

Obfuscators, on the other hand, use the so-called \textit{dead code insertion} technique in an attempt to make the code harder to follow. This technique performs the inverse operation with respect to \textit{dead code elimination}, adding dead code in the original program's code.

However, when used alone, this techniques produces an obfuscated program that can be efficiently de-obfuscated by using the compiler's dead code elimination optimization.

\paragraph{Arithmetic Substitution via Identities}
This technique aims at replacing certain operators with combinations of other operators with equal net result. Exploiting the equivalence between different combinations of different operators the code can be changed arbitrarily without changing the effective program operation and final result \cite{DangPRE}. Here are some examples of operators equivalences:

\begin{lstlisting}[caption={Operators equivalences}, label=OperatorEquivalences, language=C, style=mystyle]
	-x == ~x + 1
	
	x-1 == ~-x
	
	x+1 == -~x
	
	rotate_left(x,y) == (x << y) | (x >> (bits(x) - y))
	
	rotate_right(x,y) == (x >> y) | (x << (bits(x) - y))
\end{lstlisting}

\paragraph{Register Reassignment}
Another simple obfuscation technique is called \textit{register reassignment}. An obfuscator using this technique switches the registers used throughout the code at every application, while keeping the same program code and behaviour \cite{YouMalwareOT}.

An analyst/attacker using wildcard searching, however, easily defeats this technique.

\paragraph{Instruction Substitution}
\textit{Instruction substitution} creates variants of a program's original code by replacing some instructions with other equivalent ones \cite{YouMalwareOT}.

\paragraph{Pattern-Based Obfuscation}
\textit{Pattern-based} obfuscation is another commonly used technique similar in principle to \textit{instruction substitution}, but more complex. It consists in constructing \textit{patterns} (transformations) that map single or multiple adjacent instructions into a more complex, computationally equivalent, sequence of instructions \cite{DangPRE}.

For example, the sequence \ref{PatternObfuscationOriginalSequence} might be converted into \ref{PatternObfuscationPattern1}, as well as into \ref{PatternObfuscationPattern2} or even \ref{PatternObfuscationPattern3}.

\begin{lstlisting}[caption={Original sequence}, label=PatternObfuscationOriginalSequence, language={[x86masm]Assembler}, style=mystyle]
	push	reg32
\end{lstlisting}

\begin{lstlisting}[caption={Obfuscation using pattern 1}, label=PatternObfuscationPattern1, language={[x86masm]Assembler}, style=mystyle]
	push	imm32
	mov		dword ptr [esp],	reg32
\end{lstlisting}

\begin{lstlisting}[caption={Obfuscation using pattern 2}, label=PatternObfuscationPattern2, language={[x86masm]Assembler}, style=mystyle]
	lea		esp,	[esp-4]
	mov		dword ptr [esp],	reg32
\end{lstlisting}

\begin{lstlisting}[caption={Obfuscation using pattern 3}, label=PatternObfuscationPattern3, language={[x86masm]Assembler}, style=mystyle]
	sub		esp,	4
	mov		dword ptr [esp],	reg32
\end{lstlisting}

Moreover, patterns can be arbitrarily complicated. For example a listing such as \ref{PatternObfuscationOriginalSequence2}, could be substituted by the more complex \ref{PatternObfuscation2Pattern}.

\begin{lstlisting}[caption={Original sequence \#2}, label=PatternObfuscationOriginalSequence2, language={[x86masm]Assembler}, style=mystyle]
	sub		esp,	4
\end{lstlisting}

\begin{lstlisting}[caption={Obfuscation of sequence \#2}, label=PatternObfuscation2Pattern, language={[x86masm]Assembler}, style=mystyle]
	push	reg32
	mov		reg32,	esp
	xchg	[esp],	reg32
	pop		esp
\end{lstlisting}

Malware authors (and also software developers wishing to protect their intellectual property) can use hundreds of patterns in the same program. Moreover, most protections randomly apply patterns so that obfuscating the same program multiple times yields different results. On top of that, patterns can also be applied iteratively: after transforming the original code \textbf{C} into \textbf{C'} using pattern \textbf{P}, another pattern \textbf{P'} can be applied to \textbf{C'} in order to obtain \textbf{C''}, and so on.

Some patterns preserve \textit{semantic equivalence}, meaning that the CPU state will be the same when executing them or the original code. Some other patterns, however, do not. Therefore, depending on the code logic, some substitutions are safe (meaning that the program behaviour and final results are preserved) while others are not. This makes the job of an obfuscator challenging.

\subsubsection{Control-Based Obfuscation}
Standard static analysis tools generally make assumptions similar to the ones human reverse engineers make when analysing code. Compilers, in fact, predictably translate control flow constructs and data structures. As a result, reverse engineers (and static analysis tools) can easily recognise the original code high level control flow. \textit{Control-based obfuscation} transforms the code control flow structures in non standard ways in order to complicate both static and dynamic code analysis \cite{DangPRE}.

Some examples of stanrard static analysis tools assumptions are:
\begin{itemize}
	\item The \textit{CALL} instruction is always used with the sole purpose of invoking functions.
	
	\item Both sides of a conditional branch may feasibly be taken at runtime.
	
	\item Function calls almost always return.
	
	\item All control transfers target code locations, not data locations.
	
	\item Exceptions are used in standard and predictable ways.
	
	\item etc..
\end{itemize}

By violating these assumptions, \textit{control-based obfuscation} techniques confuse disassemblers and other static analysis tools making the analysis more difficult.

\paragraph{Functions In/Out-Lining}
Reverse engineers frequently rely on control-flow and call graphs to better understand a program's high-level logic. In particular a call graph represents calling relationships between subroutines (functions) in a computer program. Each node of a call graph represents a procedure and each edge (\textbf{f}, \textbf{g}) indicates that procedure \textbf{f} calls procedure \textbf{g}. By making the call graph harder interpret, obfuscators can hinder the reverse engineers capability of understanding the program behaviour \cite{DangPRE}. To do so one could:

\begin{itemize}
	\item \textbf{Inline functions}.
	The code belonging to a subfunction is merged into the code of its caller. If a subfunction is called multiple times, however, the code size can quickly grow.
	
	\item \textbf{Outline functions}.
	A subpart of a function is extracted and transformed into an independent function and replaced by a call to the newly created function.
\end{itemize}

Using these two operations in combination on a program's code results in a degenerated call graph with no clear logic. Moreover, also the functions' prototypes can be modified adding extra fake arguments, reordering arguments and so on, to further hide the high-level logic.

\paragraph{Destruction of Sequential and Temporal Locality}
Usually, in non-obfuscated code, the instructions of a single basic block lie one after the other (\textit{sequential locality}), and basic blocks related to one another (such as successive blocks) are close to each other (\textit{sequential locality of temporally related code}). This is done in order to maximize the instruction cache locality and reduce the number of branches in the final code. Reverse engineers thus can usually rely on the fact that all the code responsible for a specific operation will reside in a single region \cite{DangPRE}.

Violating this assumption introducing unconditional branches that break sequential locality and temporal locality of multiple basic blocks makes manual analysis more difficult. However, by constructing the control-flow graph and removing spurious unconditional branches the original control flow can be restored.

\paragraph{Processor-Based Control Indirection}
Instructions like \textit{JMP} (branch) and \textit{CALL} (save instruction pointer and branch) are, for most processors, the 2 essential control flow transfer primitives. In order to make analysis more difficult, one could obfuscate these primitives for example using dynamically computed branch addresses or by emulating them.

For example the instruction \textit{JMP} instruction \ref{ProcessorBasedControlIndirectionBefore}, can be replaced by the (almost) semantically equivalent listing \ref{ProcessorBasedControlIndirectionAfter}.

\begin{lstlisting}[caption={Processor-based control indirection before}, label=ProcessorBasedControlIndirectionBefore, language={[x86masm]Assembler}, style=mystyle]
	jmp		target_addr
\end{lstlisting}

\begin{lstlisting}[caption={Processor-based control indirection after}, label=ProcessorBasedControlIndirectionAfter, language={[x86masm]Assembler}, style=mystyle]
	push	target_addr
	ret
\end{lstlisting}

\paragraph{Operating System-Based Control Indirection}
As already seed when talking about anti-disassembler techniques, obfuscation can also exploit operating system primitives and structures. For example, the Structured Exception Handler (\textit{SEH}), Vectored Exception Handler (\textit{VEH}) and Unhandled Exception Handler are commonly used to obfuscate the control flow of Microsoft Windows executables (in Unix-like systems the signal handlers \textit{setjmp} and \textit{longjmp} are commonly used instead) \cite{DangPRE}.

\paragraph{Subroutine Reordering}
\textit{Subroutine reordering} in an obfuscation technique that randomly changes the order of a program's subroutines in the original code. This technique can thus generate $n!$ code variations, where $n$ is the number of subroutines \cite{YouMalwareOT}.

\paragraph{Opaque Predicated}
An \textit{opaque predicate} is a non-trivial boolean expressions with a constant result (always true or always false) known only at compilation/obfuscation time. Combining it with a conditional \textit{jmp} instruction introduces an additional branch in the control flow graph (\textit{CFG}). We already briefly talked about this specific combination when talking about the \textit{Jump instruction with a constant condition} anti-disassembly technique. The added branch should look as real as possible in order to elude detection, and it can be used to insert junk code or to form cycles in the control-flow graph to better hide the original program's logic \cite{DangPRE}.

\subsubsection{Simultaneous Control-Flow and Data-Flow Obfuscation}
\textit{Data-flow obfuscation} and \textit{Control-flow obfuscation} techniques are commonly used together to complicate analysis.

\paragraph{Inserting Junk Code}
This technique consists in introducing a dead code block (meaning that it will never be executed at runtime) between two other code blocks. Typically used in conjunction with \textit{opaque predicates}, this technique is used to hinder a disassembler that is disassembling an invalid path. Moreover, the junk code typically contains partially invalid instructions, or branches to invalid addresses with the objective of over-complicating the CFG \cite{DangPRE}.

\begin{lstlisting}[caption={Junk Code example}, label=JunkCodeExample, language={[x86masm]Assembler}, style=mystyle]
	push	eax
	xor		eax,	eax
	jz		9
	;<junk code start>
	jg		4
	inc		esp
	ret
	;<junk code end>
	pop		eax
\end{lstlisting}

The listing \ref{JunkCodeExample} presents an example of this technique. More precisely the instruction at line 2 (\textit{xor eax, eax}) zeroes the \textit{EAX} register setting clearing the zero flag (it is set to 0); therefore the conditional jump (\textit{jz 9}) at line 3 is always taken at runtime. The immediately next instructions are therefore junk code.

\paragraph{Control-Flow Graph Flattening}
\textit{Control-flow graph flattening} consists in replacing all control structures within a sub-part of the control flow graph with a single switch statement commonly called \textit{dispatcher}. This is done to hide the true basic blocks relationships within the dispatcher. When using this technique, first a subpart of the program's control flow graph is selected to be substituted by the dispatcher. Some transformations may then be applied to the basic blocks inside the chosen sub-graph (they split or merged) to further complicate analysis and finally each basic block updates the dispatcher's context to reflect the relative basic block relationships. The final resulting graph offers no clues about the structure of the algorithm, but has the same logic \cite{DangPRE}.

\textit{CFG flattening} is frequently used, together with \textit{opaque predicates}, to insert dead code paths in the CFG.

\paragraph{Code Transposition}
An obfuscator using the technique called \textit{code transposition} effectively reorders the sequence of a program's original code instructions without changing its behaviour \cite{YouMalwareOT}. To achieve this two approaches are commonly followed:
\begin{itemize}
	\item Randomly shuffling the instruction and then recovering the original execution order by inserting unconditional branches. This is easily defeated restoring the original program by removing (and following) the unconditional branches.
	
	\item Choosing and reordering independent instructions that have no impact on the others. This approach is harder to implement given the complexity of finding independent instructions, but it is more effective.
\end{itemize}

\paragraph{Code Virtualization}
\textit{Code virtualization} consists in transforming a program's binary code (compiled for a specific machine) into a different binary code that is understood by a virtual machine. More specifically, the instruction set from the source machine is converted into a new instruction set, understood by the target virtual machine. This can be done using multiple types of virtual machines with different instruction sets. This means that a specific block of, for example, Intel x86 instructions can be converted into a different instruction set for each machine, preventing an analyst/attacker from recognizing any generated virtual opcode after the transformation from x86 instructions \cite{DangPRE}. 

Usually, some specific blocks of the program's code are virtualized (and not the whole program) and inserted back into the program alongside the associated interpreter. At run time, the interpreter assumes execution control and translates the virtualized code back to the original byte code.

When an analyst/attacker tries to decompile a virtualized block of code, however, he will not find the original x86 instructions. Instead, he will find a completely new instruction set which is not recognized by him or any other special decompiler. This will force the attacker to identify how each opcode is executed and how the specific virtual machine works for each protected application. 

Some examples of code virtualization tools include \href{https://vmpsoft.com/}{VMProtect} and \href{https://www.oreans.com/CodeVirtualizer.php}{CodeVirtualizer}.

\paragraph{Code Integration}
\textit{Code integration}, one of the most sophisticated obfuscation techniques, was first introduced by \textit{Win97/Zmist} malware. A malware using this technique first decompiles the target program into a set of manageable objects, it then inserts itself between them and finally it reassembles the code \cite{YouMalwareOT}.

\subsection{Obfuscated Malware}
The huge amount of malware released in the wild since the creation of the first virus in 1960s can be split into two generations. More specifically, the first generation malwares were static, their code and behaviour did not change. The more sophisticated second generation malwares, on the other hand, change their internal structure between one variant and the other maintaining the same malicious behaviour in order to avoid detection.

\subsubsection{Encrypted Malwares}
The first second-generation malwares ever existed exploited encryption in order to evade detection by signature-based antivirus scanners.

In this approach, an encrypted malware typically consists of two parts: the encrypted main body and a decryption code (also called \textit{decryptor}). The \textit{decryptor}'s objective is to recover the original malware code from the encrypted body whenever the infected file is run \cite{YouMOT}.

Moreover, to hide from signature-based scanners, encrypted malware encrypts its code using a different key at each infection, thus creating a unique encrypted body. The decryption routine (\textit{decryptor}), however, remains the same from one generation to another. This means that encrypted malwares can be detected with signature-based scanners by searching for the decryptor's code pattern \cite{Sharma_2014}.

The first known malware to exploit encryption for detection evasion was CASCADE which spread in the 1980s and early 1990s.

Encryption of malware code is often used in conjunction with the use of packers.

\color{Black}
\subsubsection{Packed Malwares}
Malware authors are nowadays increasingly exploiting packers (or even multiple packers at once) to produce numerous variants of the same original malware code \cite{NamanyaTWM}.

As stated by Perdisci, et al \cite{PerdisciCPEACVD}, more than 80\% of the new malware currently discovered are actually packed versions of already existing malware.

Packers are used to compress the original file into a smaller size and, moreover, encryption is sometimes applied to the compressed version of the file in order to make the unpacking process more difficult.

However, it is not uncommon to see malware authors writing and using custom packers. This fact can be used by analysts to detect if a file is malicious, without further analysis, based on the fact that benign software vendors would almost never use custom packers. On the other hand, many malware authors frequently use commercial, and readily available, packers to generate malware variants.

\subsubsection{Oligomorphic Malwares}
\color{Red}

Also called 'Semipolymorphic' malware, they employ multiple decryption routines which are chosen randomly at infection as a way of avoiding signature based detection \cite{NamanyaTWM}. The Whale virus was the first malware to use this technique and it carried a few dozens of different decryptors and picked one randomly.

The short comings of the encrypted malware led to the development of different concealment techniques \cite{Sharma_2014}. In Oligomorphic malwares decryptors are mutated from one variant to another. Initially this type of malware was capable of changing the decryptor just slightly. the simple method to create Oligomorphic malwares is to provide a set of different decryptors rather than just one. At most this malware can generate few hundreds different decryptors, e.g. Win95/Memorial had the ability to build 96 different decryptor patterns. For its detection, signature based techniques can be applied by making the signature of all the decryptors. However, in general to detect Oligomorphic malwares, signature based techniques are not a good approach.

In order to address the shortcomings of the encrypted malwares, malware authors devised technologies, through which malwares can mutate their decryptor from one generation to the next \cite{YouMOT}. The first attempt was the oligomorphic malware capable of changing its decryptor slightly. However, this malware can generate at most a few hundreds of different decryptors, thus still being able to be detected with signatures.

\color{Black}
\subsubsection{Polymorphic Malwares}
\color{Red}

To overcome the oligomorphic malware limitations, malware authors developed the polimorphic malware \cite{YouMOT}. The polymorphic malware achieves to create countless numbers of distinct decryptors with the help of the obfuscation methods including dead-code insertion, register reassignment, and so forth. Especially, due to the powerful toolkits such as '\textit{The Mutation Engine (MtE)}', it was a critical problem. The toolkits help the malware writers to easily convert their non-obfuscated malware into the polymorphic version. Even though the polymorphic malwares can effectively thwart the signature matching, their constant body, which appears after decryption, can be used as an important source for detection. In order to exploit this vulnerability, antivirus tools adopt the emulation technique. Through this technique, the tools execute a malware in an emulator (called '\textit{Sandbox}') without resulting in any harm. Once the constant body is loaded into memory after being decrypted, the conventional detection, \textit{i.e.}, signature based, can be applied. In order to detect and prevent such emulation, the polymorphic malwares used the armoring technique. Howeverm as the antivirus scanners became matures, they were capable of addressing this technique, thus effectively defeating the polymorphic malwares.

In Polymorphic malwares, millions of decryptors can be generated by changing instructions in the next variant of the malware to avoid signature based detection \cite{Sharma_2014}. It also consists of two parts; the first part is the code decryptor to decrypt the second part (body). During the execution of the malware, the mutation engine creates a new decryptor which is joined with the encrypted malware body to construct a new variant of the malware. Polymorphic malwares are created by using obfuscation techqnieus (dead-code insertion, register reassignment, subroutine reordering, instruction substitution, code transposition/integration etc.). The first known polymorphic malware was \textit{1260}, written in 1990. Although a large number of variants of decryptors can be created, still signature scanning techniques can be used to detect the malwares by identifying the original program with emulation techniques.

Polymorphic malware, like oligomorphic malware, use decryption routines to change the look of the execution codes for every infection \cite{NamanyaTWM}. They have a wide range of decryption engines since they tend to use mutation engines. The mutation engines perform all the logic computations in rearranging the code to prevend detection by signature matching. The decryptor is run first once the malware is copied to the machine and it enables the execution of the malware. When the malware replicates itself, it encrypts the new malware with a different key and encloses the new decryption routine in the new code. It can however only generate up to a few hundred decryptors so it can be detected.

\color{Black}
\subsubsection{Metamorphic Malwares}
\color{Red}

The metamorphic malware was proposed as a novel approach beyond the oligomorphic and polimorphic ones \cite{YouMOT}. Note that this malware makes best use of obfuscation techniques to evolve its body into new generations, which look different but work essentially the same. For such an evolution, it should be able to recognize, parse and mutate its own body whenever it propagates. It is important that the metamorphic malware never reveals its constant body in memory due to not using encryption or packing. This makes it so difficult for the antivirus scanners to detect this malware.

Metamorphic malwares are body-polymorphic, i.e. instead of generating new decryptors, a new instance (body) is created without changing its actions \cite{Sharma_2014}. Similarly to polymorphic malwares, obfuscation techniques can be used to create new instances. It is believed that in future it will harm both computers and PDAs in large scale as it is almost impossible to detect by signature based techniques. Creating a true metamorphic malware without arbitrarily increasing the code size is a challenging task. It has been shown that there are only few malwares that exhibit true metamorphic behaviour, e.g. Phalcon/Skism Mass-Produced Code Generator, Second Generation virus generator, Mass Code Generator and Virus Creation Lab for Win32 were claimed to be metamorphic but were not. The first true metamorphic virus was created in 1998 and was called Win95/Regswap. In 2000, Win32/Ghost virus was created with 3'628'800 different variants. One of the strongest metamorphic malware, W32/NGVCK, was created in 2001 with the help of Next Generation Virus Creation Kit (NGVCK).

The malicious code body is changed instead of appearance by using a combination of various obfuscation techniques. By using dead-code insertion, register reassignment and code transposition, the body of the code is changed into a new generation but it works the same way \cite{NamanyaTWM}. This way, every generated variation of the malware looks different and therefore signature generation and signature based detection is very hard. Unlike most polymorphic malware which decrypt to a single constant code body in memory, metamorphic malware can have varying codes which makes their detection in memory rely on algorithmic scanning. Metamorphic malware can also insert and interweave their code into the host program which makes the malware harder to detect.

\color{Black}
\chapter{Detection Techniques}
\color{Red}

To combat the threat/attacks from malwares, softwares (anti-malware) are developed, which are mostly based on the assumption that the malware structure does not change appreciably. But the variant of second generation malwares are very much different to each other, hence threats/attacks from such malwares to Computers and PDAs are increasing day by day. Therefore, there is a need that both academia and anti-malware developers should continually work to prevent damage from the evolution of malwares \cite{Sharma_2014}.

Malware detection is the process of identifying malicious code from benign code so that the system can be protected or recovered from any effects of the malicious code \cite{NamanyaTWM}.

\color{Black}
\section{Integrity Checker}
\color{Red}

Compromising a computer system or network requires some changes to be made within the target environment. Integrity checkers are used in intrusion detection on the premise that a file which exists within the uncompromised operating environment is used as a measure to counter any future changes. A hashing function like the \textit{md5} sum, \textit{Sha1} or \textit{Sha256} is used to calculate the digest of the program or file and stored in the database. Later, the figests of the program/file are re-calculated and then compared against the originally calculated hash to check if the file has been modified. The challenges that this method presents are:
\begin{itemize}
	\item The system initially used to calculate the stored hashes must be deemed clean which is hard to guarantee.
	
	\item System updates and patches do modify system files and programs which means that the database of hashes needs to be updated for every update or there will be very high false positives affecting the detection method.
	
	\item There is the need to ensure that the reference database of hashes is stored offline and safely otherwise it presents a single point of failure in the detection mechanism.
\end{itemize}

Integrity checking is considered quite important in detection of any system modifications. It is however more of an incident recovery process method rather than a malware infection prevention method \cite{NamanyaTWM}.

\color{Black}
\section{Signature-based Detection}
\color{Red}

Signature based detection is the simplest and an effective way of detecting known malwares \cite{Sharma_2014}. Once the malware is identified, unique sequences of bytes are extracted from it, which represents the signature of the malware. These signatures are selected long enough to characterize a specific malware with respect to any other benign program. This technique scans the files in the system to find the defined malware signature, if found an alert of the presence of malware is sent, e.g. Aho-Corasick \cite{} algorithm scans fo the exact matching, hence a slight mismatch will escape detection. Veldamna and Wu-Manber \cite{} proposed the use of wildcards for detecting slight variances in the malwares. Some metamorphic malwares could be detected using the wildcard method, e.g. W32/Regswap. It is easy to use, however requirement of scanning becomes costly as the database of malware signatures is increasing very fast. Also, it's a completely reactive technique, therefore unable to combat threats/attacks from the new malwares until it causes damage. Gartner \cite{} believes that eventually Signature-based techniques will be replaced by more robust approaches, because today the signature-based anti-malwares have marginal value as second generation malwares can easily escape detection.

Most of the anti-virus software uses signature based detection which is inefficient in the present scenario due to the rapid increase in the number and variants of malware. The signature is a unique identification of a binary file, which is created by analysing the binary file typically using static analysis methods. Dynamic analysis uses the behaviour and actions while in execution to identify whether the executable is a malware or not. Both methods have their own advantages and disadvantages.

Signature based detection uses specific byte code sequences that are identified to be unique to a sample of malware in a specific family or variant to detect the presence of similar coded files in a system. The unique byte of code sequences are saved in the anti-virus database as signatures and are developed by a group of malware experts after detailed analysis of a significant number of malwares. Any file being scanned by the antivirus that is found to contain the signature of the unique byte code sequence is deemed to be malicious. This implies that a database of signatures must be maintained by the antivirus and updated every time new signatures are generated in order to detect new malware. This creates one of the major challenges faced by system users as updating these signatures requires access to networking resources that might not be readily available all the time \cite{NamanyaTWM}.

Signature detection is the simplest method and is the most widely used for traditional malware detection. This method constructs a database that contains signatures of all known malware. When analysing a new programming code, it compares the signature of the analysed virus with its database, if the matching is found, the analysed file is considered as virus. This approach is fast and has high positive rate, however the database needs to be updated with new signatures. Although this technique is old it was used in the early days of polymorphic detection when investigators/researchers analysed the virus manually, one by one, line by line to detect various sequences of programming codes. As the number of viruses has been increasing so fast, this technique quickly became time-consuming, expensive and impractical \cite{NguyenSPVD}.

\color{Black}
\subsection{Yara Rules}
\color{Red}
YARA is a popular open-source tool that provides a robust language, which is compatible with Perl-based Regular Expressions, and is used to examine the suspected files/directories and match strings as is defined in the YARA rules with the file \cite{NinjaYSEWDM}.
Each rule has to start with the word "\textit{rule}", followed by the rule identifier (or RuleName). \textbf{RuleName} is the identifier of the rule. Identifiers must follow the same lexical conventions of the \textit{C} programming language: they can contain any alphanumeric character and the underscore character, but the first character cannot be a digit. Rule identifiers are case sensitive and cannot exceed 128 characters. Moreover there is a list of YARA keywords that are not allowed to be used as an identifier because they have a predefined meaning.
The main body of YARA rules contains three sections:
\begin{enumerate}
	\item \textbf{Meta}: The meta section is found at the top of a rule and can contain anything the author want to convey about the rule. Meta tags like "author" and "description" are often found to five the author credit and let others know what the rule is meant to accomplish. When creating rules based on malware, authors will sometimes leave tags with file hashes or references to blogs with more information as well \cite{SimonWIY}.
	
	Metadata can be added to help identify the files that were picked up by a certain rule. The metadata identifiers are always followed by an equal sign and the set value. The assigned values can be strings, integers, or a Boolean value. Note that identifiers/value pairs defined in the metadata section can't be used in the condition section, their only purpose is to store additional information about the rule \cite{ArntzEYR}.
	
	\item \textbf{Strings}: This section contains the strings/patterns/signatures that we need to match against a file. The strings section is optional and can be left out if not necessary. In YARA there are 3 types of strings named as follows:
	\begin{itemize}
		\item \textit{Hexadecimal Strings}: Hexadecimal Strings will match hexadecimal characters in the output file. This allows three special instructions such as wildcards, jumps and alternatives.
		\begin{itemize}
			\item \textit{Wildcard}: This is represented by a '\textit{?}' and it indicates that some bytes in the pattern are unknown and should match anything. For example:
			\begin{lstlisting}[caption={YARA Hexadecimal Wildcard}, label=YaraWildcard, language=C, style=mystyle]
				$hex_example = {B1 B2 ? ? B8}
			\end{lstlisting}
			
			\item \textit{Jumps}: In circumstances when we know the values of the pattern but their length varies then we can use jumps. For example:
			\begin{lstlisting}[caption={YARA Hexadecimal Jump}, label=YaraJump, language=C, style=mystyle]
				$jump_example = {F1 F2 [2-3] 24}
			\end{lstlisting}
			this indicates that any arbitrary sequence from 2 bytes to 3 bytes can occupy the sequence.
			
			\item \textit{Alternatives}:
		\end{itemize}
		
		\item \textit{Text Strings}: Text strings are in form of ASCII text which is then matched up with the condition set. This section also contains further modifiers like "wide" or "nocase":
		\begin{itemize}
			\item \textit{Case Sensitive Strings}: Example:
			\begin{lstlisting}[caption={YARA Case Sensitive Strings}, label=YaraCaseSensitive, language=C, style=mystyle]
				$text_case_example="test"
			\end{lstlisting}
			
			\item \textit{Case Insensitive Strings}: Example:
			\begin{lstlisting}[caption={YARA Case Insensitive Strings}, label=YaraCaseInsensitive, language=C, style=mystyle]
				$text_nocase_example="test" nocase
			\end{lstlisting}
			
			\item \textit{Wide Character Strings}: Example:
			\begin{lstlisting}[caption={YARA Wide Character Strings}, label=YaraWideCharacter, language=C, style=mystyle]
				$text_wide_example="test" wide
			\end{lstlisting}
		\end{itemize}
	
		\item \textit{Regular Expressions}: Starting from version 2.0 YARA has its own regular expression engine, which mostly resembles PCRE. YARA regular expression can be followed by ant of the text strings mentioned above.
	\end{itemize}

	\item \textbf{Conditions}: Conditions sets evaluate Boolean expressions. For example, in the main example above, it evaluates either of \textit{\$testy\_string1} or \textit{\$test\_string2} to be true.
	The \textit{condition} section is the only one that is required. This section specifies when the rule result is \textit{true} for the \textit{object} (file) that is under investigation. It contains a boolean expression that determines the result. Conditions are by design Boolean expressions and can contain all the usual logical and relational operators \cite{ArntzEYR}.
	Under the Condition set, we can:
	\begin{itemize}
		\item Count the string presence:
		\begin{lstlisting}[caption={YARA Count String Presence}, label=YaraCountPresence, language=C, style=mystyle]
			#test_string1=2 and #test_string2<10
		\end{lstlisting}
		
		\item String Offset: This is used to find out if a particular string is available at a specified offset of the running process. This is further achieved by following keywords:
		\begin{itemize}
			\item \textit{at}:
			\begin{lstlisting}[caption={YARA String Offset At}, label=YaraStringOffsetAt, language=C, style=mystyle]
				$test_string1 at 200 and $test_string2 at 500
			\end{lstlisting}
			this will find whether the test\_string1 is located at offset 200 of the running process and test\_string2 at offset 500 of the running process.
			
			\item \textit{in}: this is used when we want to define a range of memory locations where we need to search the string. For example:
			\begin{lstlisting}[caption={YARA String Offset In}, label=YaraStringOffsetIn, language=C, style=mystyle]
				$test_string1 in (100 .. 200)
			\end{lstlisting}
			will find the test\_string1 in the memory location between \textit{100} and \textit{200} of the running process.
		\end{itemize}
		
		\item Check file size: Example:
		\begin{lstlisting}[caption={YARA Check File Size}, label=YaraCheckFileSize, language=C, style=mystyle]
			filesize>10000
		\end{lstlisting}
		
		\item Set of strings: Example:
		\begin{lstlisting}[caption={YARA Set of Strings}, label=YaraSetOfStrings, language=C, style=mystyle]
			2 of ($test_string1, $test_string2, $test_string3)
		\end{lstlisting}
		this will say at least two of the strings enclosed must match with the file.
	\end{itemize}
\end{enumerate}
Some interesting Yara use cases \cite{NinjaYSEWDM}:
\begin{itemize}
	\item \textbf{YARA with PE}
	Starting with version 3.0, YARA can parse Portable Executable (PE) files. For example the following rule will parse the PE file and look for the import section of the PE file along with the strings:
	\begin{lstlisting}[caption={YARA with PE}, label=YaraPE, language=C, style=mystyle]
		Import "PE"
		
		Rule PE_Parse_Check
		{
			strings:
				$string_pe="abc" nocase

			condition:
				pe.imports("Kernel32.dll", "CreateProcess") and
				pe,imports("wininet.dll", "httpsendrequest") and
				$ string_pe
		}
	\end{lstlisting}
	This rule, \textit{PE\_Parse\_Check}, will check for the string "abc" and match it with PE import statements looking out for a "process creation" and "http send request".
	
	\item \textbf{YARA with PEiD}
	YARA can also be integrated with \textit{PEiD} to check what packer was used to compile the malicious/suspected executable.
\end{itemize}


YARA can be described as a tool aimed at helping malware researchers to identify and classify malware samples. YARA allows anyone to create "descriptions" known as rules based on text or binary patterns. Although it can be used to detect patterns for any type of purpose, it is most often used for the detection of malware \cite{SimonWIY}.

\textbf{Strings and Modifiers}
YARA supports searching for multiple string types but will default to ASCII when the encoding is not specified. To specify how YARA should search for strings, we can add a modifier at the end of the string definition. For example, keywords like "\textbf{ascii}" or "\textbf{wide}" could be added to the end of a string definition. Many of these modifiers can even be used together to keep rules simple and readable.
In addition to text encodings like ASCII or wide-character strings, YARA can also encode text for you before searching. Other modifiers like "\textbf{base64}" and "\textbf{XOR}" allow the author to write a plain-text string like "hello" and then base64 or XOR encode it before searching. Again, this makes rules more readable, as it is obvious what the author wants to fine without having to first decode what gibberish such as "\textbf{aGVsbG8=}" or "\textbf{*'..-}" means in the rule ("hello" base64 encoded and XOR encoded with a key of \textit{0x42}, respectively) \cite{SimonWIY}.

\textbf{Rule Conditions}
For YARA to be useful, the rule author must tell it exactly what they want to find. This is done in the "condition" section at the end of the rule. Conditions can be simple or more complicated expressions \cite{SimonWIY}.



Yara is widely used to specify signatures and perform searches. Yara is a tool to combine content matching against simple regular expressions with logic rules, and these rules 'fire' if the predicates are satisfied. These predicates combined are often called 'Yara Rules', and may be used to identify specific malware families, the presence of CVEs, specific signatures of functionality, or generic indicators of maliciousness. Developing effective Yara rules can be very time intensive, especially for junior analysts who lack deeper expertise and intuition on what should be included in a Yara rule to achieve a goal. For example, a related task in performing reverse engineering (a task that may be necessary to build good signatures for difficult malware samples) can take several hours to weeks for a single file, even for expert users with over a decade of experience.
We consider the problem of trying to develop a Yara rule to identify a specific malware family given only a limited number of example files from that family.
A common workflow in developing Yara rules is to manually inspect multiple files to determine common contents, wrapped by trial-and-error refinement of the developed rules, where success is measured against coverage and false positive rates on a collection that includes benign or out-of-family files \cite{RaffAYRGUB}.

Yara is an industry standard regular expression tool designed for malware analysis. Many malware analysis tools support Yara directly \cite{RaffAYRGUB}.


Emerging as a widely accepted technique for malware analysis, YARA rules, due to its flexible and customisable nature, allows malware analysts to develop rules according to the requirements of a specific security domain. YARA rules can be automatically generated using tools, however, they may require post-processing for their optimisation, and may not be effective for the specific security domain \cite{NaikEAGYRETE}.

In recent years, YARA rules technique has emerged as a widely accepted technique for malware analysis due to its flexible and customisable nature, allowing malware analysts to develop YARA rules according to their specific requirements in targeting specific types of threats. YARA rules are generated based on reverse engineering of malware samples to include the most common \textit{Indicator of Compromise} (\textit{IoC}) strings from those malware samples to find similar types of malware.
The success of YARA rules is dependent on the effectiveness of generated YARA rules, which is determined by the types of IoC strings and the number of IoC strings utilised in its rules. Therefore, the generation of the most effective YARA rules is the biggest challenge in applying YARA rules for malware analysis.
YARA rules can be generated either manually or automatically. Generating YARA rules manually requires a highly-specialised skill-set in a specific security area, whereas generating YARA rules automatically using a tool is a relatively easy task. However, there are several issues with automatically generating YARA rules such as these rules require post processing operations for their optimization, despite this they may not become very effective for certain types of threats \cite{NaikEAGYRETE}.

YARA rules are developed to detect malware by matching its signatures/strings with the existing malware signatures/strings. These rules contain predetermined signatures/strings related to known malware used in attempting to match against the targeted files, folders, or processes. YARA rules consist of three sections: meta, strings and condition. Here, strings can be classified into three types: text strings, hexadecimal strings and regular expression strings. Text strings are generally a readable text complemented with some modifiers (e.g. nocase, ASCII, wide, and fullword), to manage the process more effectively. Hexadecimal strings are a sequence of raw bytes complemented with three flexible formats: wild-cards, jumps, and alternatives. Regular expression strings are similar to text strings as a readable text complemented with some modifiers; which are available since version 2.0 and increase the capability of YARA rules. Text strings and regular expressions which express a sequence of raw bytes through the use of escape sequences. The final part of YARA rules is a rule condition that specifies the target to declare the sample as malware. YARA conditions determine whether to trigger the rule or not, however, these conditions are Boolean expressions similar to those used in all other programming languages \cite{NaikEAGYRETE}.
\begin{lstlisting}[caption={YARA Rules Syntax}, label=YaraSyntax, language=C, style=mystyle]
	rule RuleName
	{
		meta:
			description = "description of rule"
			author = "name"
			date = "dd/mm/yyyy"
			reference = "url"
		
		strings:
			$text_string1 = "text1 you wish to find in malware"
			$text_string2 = "text2 you wish to find in malware"
			
			$hex_string1 = {hex1 you wish to find in malware}
			$hex_string2 = {hex2 you wish to find in malware}
			
			$reg_exp_string1 = /regular expression1 you wish to find in malware/
			$reg_exp_string2 = /regular expression2 you wish to find in malware/
			
		condition:
			$text_string1 or $text_string2 or
			$hex_string1 or $hex_string2 or
			$reg_exp_string1 or $reg_exp_string2
	}
\end{lstlisting}

\begin{lstlisting}[caption={YARA Rules Example}, label=YaraExample, language=C, style=mystyle]
	rule WannaCry
	{
		meta:
			description = "Generic Signature of WannaCry"
			author = "Nitin Naik"
			date = "01/06/2018"
			reference = "www.mydomain.com"
		
		strings:
			$text_string1 = "encrypt"
			$text_string2 = "bitcoin"
		
			$hex_string1 = {B6 D3 56 A5 78 43}
			$hex_string2 = {E8 27 F9 83 C4 82}
			
			$reg_exp_string1 = /md5:[0-9a-fA-F]{32}/
			$reg_exp_string2 = /state:(on|off)/
		
		condition:
			$text_string1 or $text_string2 or
			$hex_string1 or $hex_string2 or
			$reg_exp_string1 or $reg_exp_string2
	}
\end{lstlisting}

\begin{enumerate}
	\item \textit{Advantages} of YARA rules:
	YARA rules offer several advantages over other malware analysis techniques, here are some of the most notable advantages \cite{NaikEAGYRETE}:
	\begin{itemize}
		\item YARA rules offer an easy and efficient way of writing flexible and custom rules according to the requirements of a specific security domain.
		
		\item YARA rules are an open standard and work on most of the major platforms such as Windows, Linux and Mac OS.
		
		\item YARA rules can be easily integrated into Python and C/C++ programming languages.
		
		\item YARA rules can be used for both static and dynamic malware analysis.
		
		\item Several tools are available to generate YARA rules easily and efficiently.
		
		\item Several public repositories of YARA rules offer readily available rules for malware analysis.
	\end{itemize}
	
	\item \textit{Limitations} of YARA rules:
	YARA rules are one of the most established malware analysis techniques, however, they have some limitations, here are some of the most notable \cite{NaikEAGYRETE}:
	\begin{itemize}
		\item YARA rules are commonly written based on \textit{IoC} strings, however, attackers can easily manipulate, replace or encrypt these \textit{IoC} strings to evade them, which could make these rules less effective.
		
		\item \textit{IoC} strings are extracted from existing malware and their families through a reverse engineering process, which requires a highly specialised skill-set in a specific security domain.
		
		\item The success of YARA rules is dependant on the types and number of \textit{IoC} strings included in rules, however, achieving the balance of both is a challenging task as an ineffective and inappropriate number of \textit{IoC} strings could affect the performance of YARA rules.
		
		\item YARA rules are effective in detecting malware which resembles similarity with the existing malware and their families, however, it may miss out new and unique malware variants.
	\end{itemize}
\end{enumerate}

\color{Black}

\subsubsection{YARA Rules Automatic Generators}
\color{Red}
Generating YARA rules automatically is the most popular method in employing YARA rules in malware analysis \cite{NaikEAGYRETE}.

\color{Black}
\paragraph{\textit{yarGen} Tool}
\color{Red}
\textit{yarGen} is a Python-based tool utilised to generate YARA rules, which is developed by Florian Roth \cite{}. It generates YARA rules utilising some intelligent techniques such as fuzzy regular expressions, Naive Bayes classifier and Gibberish Detector \cite{}. The generated YARA rules include those strings and opcodes from malware which do not match with the provided goodware databases. These YARA rules contain a predefined number of strings (generally up to 20 strings), based on their highest scores to maintain a reasonable operation speed. This tool generates two types of rules: basic rules and super rules, depending on the malware sample types, where basic rules can generally target specific malware and super rules can target a set of malware or malware family \cite{NaikEAGYRETE}.

\textit{YarGen} uses a Naive Bayes model to score the potential utility of features that can be extracted from a binary, predominately strings. Then YarGen uses a number of heuristics to select the features to use, and combines them in a heuristic fashion. The authors of YarGen encourage its use as a starting point for rule construction, and to build rules by manually adjusting and refining YarGen's output \cite{RaffAYRGUB}.

\begin{enumerate}
	\item \textit{Advantages} of \textit{yarGen} tool:
	\begin{itemize}
		\item It allows generation of YARA rules based on both opcodes and strings.
		
		\item It supports the use of PE (portable executable) modules, which are used by the Windows operating system for executables such as \textit{DLL} and \textit{COM} files.
		
		\item It can be integrated with other anti-malware software for its more effective use.
		
		\item It reduces false positives by checking all strings against strings of goodware databases.
		
		\item The python script is simple and easy to used through a command line interface.
	\end{itemize}
	
	\item \textit{Drawbacks} of \textit{yarGen} tool:
	\begin{itemize}
		\item It requires post-processing of rules for increasing their effectiveness.
		
		\item It requires significant resources for opcode-based rules and loading goodware files.
		
		\item The rule generation process is slow.
		
		\item The creation of super rules could cause duplication of rules and redundancy.
		
		\item It requires installation of all dependencies and built-in databases for working successfully.
	\end{itemize}
\end{enumerate}

\color{Black}
\paragraph{\textit{yaraGenerator} Tool}
\color{Red}
It is a python-based tool used for the generation of YARA rules, which is developed by Chris Clark \cite{}.It generates YARA rules with a completely different signature for different types of files such as \textit{EXE}s, \textit{PDF}s, and \textit{Email}s utilising string prioritization logic and code refactoring. The generated YARA rules consist of strings only, including those strings from malware which do not match with the provided blacklist of strings. It uses a database of 30,000 blacklisted strings divided based on different file formats. These YARA rules contain a large number of strings (depending on the types of samples) selected randomly as it does not compute a score or weighting for strings \cite{NaikEAGYRETE}.

\begin{enumerate}
	\item \textit{Advantages} of \textit{yaraGenerator} tool:
	\begin{itemize}
		\item It can generate specialised rules of a specific file format.
		
		\item It supports the use of \textit{PE} (portable executable) modules, which are used by Windows operating system for executables such as \textit{DLL} and \textit{COM} files.
		
		\item It reduces false positives by checking all strings against strings of blacklist files.
		
		\item The python script is simple and easy to use through a command line interface.
	\end{itemize}
	
	\item \textit{Drawbacks} of \textit{yaraGenerator} tool:
	\begin{itemize}
		\item It requires post-processing of rules for increasing their effectiveness.
		
		\item It generates YARA rules based on random selection of strings which may not select the most appropriate strings in many cases.
		
		\item It does not support the inclusion of opcodes.
		
		\item The project was developed as a work in progress and has not been updated afterwords.
	\end{itemize}
\end{enumerate}

\color{Black}
\paragraph{\textit{yabin} Tool}
\color{Red}

It is another python-based tool used for generating YARA rules, which was developed by \textit{Alien Vault Open Threat Exchange} (\textit{OTX}) community. It generates YARA rules by finding rare functions in certain malware samples or families. It recognises functions by checking function prologues which define the start of functions, for example, \textit{55 8B EC} mostly specifies the start of a function in programs compiled by Microsoft Visual Studio. The generated YARA rules include those strings from malware which do not match with the provided whitelist of common library functions. It uses a whitelist obtained from 100 Gb of non-malicious software to omit common library functions. These YARA rules contain a list of hexadecimal strings to compare against suspected malware files for finding the similarity in their byte-sequences \cite{NaikEAGYRETE}.
\begin{enumerate}
	\item \textit{Advantages} of \textit{yabin} tool:
	\begin{itemize}
		\item It can be used to cluster malware samples based on the reuse of their code.
		
		\item The search patterns can be extended during the post-processing operation.
		
		\item It provides a large whitelist obtained from numerous non-malicious software to omit common library functions.
		
		\item The python script is simple and easy to use through a command line interface.
	\end{itemize}
	
	\item \textit{Drawbacks} of \textit{yabin} tool:
	\begin{itemize}
		\item It requires post-processing of rules to make them more effective.
		
		\item It may not work on some specific file formats.
		
		\item It only uses functions and does not use other types of strings.
		
		\item It can only work with unpacked executables.
		
		\item It is not designed to work on \textit{.NET} executables, \textit{Java} files and \textit{Microsoft} documents.
		
		\item It is mainly developed for the testing purpose and not for production use.
	\end{itemize}
\end{enumerate}

\color{Black}
\paragraph{\textit{AutoYara} Tool}
\color{Red}
The previously mentioned tools like \textit{YarGen} rely on a number of heuristics and string features and have varying levels of effectiveness. \textit{AutoYara} tool makes larger rules, but uses the redundancy and conjunction of components to achieve the extremely low false-positive rates that analysts desire. (The two primary concerns are: 1. Yara rules that generate a lot of false positives could slow the investigation and 2. security workers often have fewer ($\leq 10$) samples when creating a Yara rule.) This task is done in two steps. First, AutoYara authors leverage recent work in finding frequent larger n-grams, for $n \leq 1024$, to find several candidate byte strings that could become features. Second, they extend the SpectraclCoClustering algorithm to work when the number of bi-clusters is not known \textit{a priori}. Bi-clustering allows AutoYara to easily produce complex logic rules that allow to build signatures with low false positive rates  \cite{RaffAYRGUB}.

AutoYara is fast, allowing for deployment on low-resource equipment for teams that deploy to remote networks \cite{RaffAYRGUB}.

Autoyara makes use of bi-clustering algorithms as the fundamental mechanism of rule construction. The goal of bi-cluster is to take an input matrix \textit{X}, and to simultaneously cluster the rows and columns of the matrix to reveal an underlying structure between columns and rows. AutoYara authors used bi-clustering because it provides a natural means to extract Yara rules. To build a good Yara rule, in fact, one needs to know 1. which features should be used at all, and 2. which features should be combined into an '\textit{and}' statement (would reduce FPs), and which should be placed into an '\textit{or}' statement (would increase TPs). Bi-clustering provides a simple approach to do this jointly over the features, rather than considering greedy approaches that select features one at a time. For every feature within a bi-cluster, they <and> them together since they co-occur, and they <or> the predicates built from bi-clusters. This results in a "disjunction of conjunctions" rule formulation. They used a bi-clustering method that can determine the number of bi-clusters automatically (most bi-clustering algorithms require specifying the number of vi-clusters in advance, and enforce no overlaps between bi-clusters), even if it is only one bi-cluster, that allows overlapping bi-clusters, and will discard rows and columns that do not fit in any bi-cluster \cite{RaffAYRGUB}.

\begin{enumerate}
	\item \textit{Advantages} of \textit{AutoYara} tool:
	\begin{itemize}
		\item It is fast, allowing for deployment on low-resource equipment for teams that deploy to remote networks.
		
		\item It was designed with the intent of producing yara rules with low false positive rates.
		
		\item It was designed to be able to generate yara rules having even $\leq10$ available samples.
	\end{itemize}
	
	\item \textit{Drawbacks} of \textit{AutoYara} tool:
	\begin{itemize}
		\item It requires post-processing of rules to make them more effective.
		
		\item It a very recent tool mainly developed for the research purposes and not for production use.
	\end{itemize}
\end{enumerate}

\color{Black}
\section{Semantic Based Detection}
\color{Red}

Semantics-based malware detection looks to identify the malware by deducing the logic of the code and matching it to already known malicious logic patterns. It follows the semantics of the code instructions within the file instead of looking at the syntactic properties unlike signature based detection. This allows for the semantic based detection approaches to overcome obfuscation and detect unknown malware variants \cite{NamanyaTWM}. 

\color{Black}
\section{Behavioural Based Detection}
\color{Red}

Behavioural-based detection techniques focus on using specific system/application behaviours and activities that are observed during dynamic analysis of the sample to form patterns that can be used to identify software that invoke similar patterns. Although these techniques are largely immune to obfuscation, their applicability is limited by their performance as dynamic analysis requires time and determining the unsafe activities and behaviours within the environment is an evolving challenge \cite{NamanyaTWM}.

\color{Black}
\section{Heuristics-based Detection}
\color{Red}

In this method there are two approaches for the detection of malwares. Firstly in static approach suspicious programs are disassembled to find a matching of the known malware pattern, if any. If the analysis result crosses the preset threshold then the program is marked as infected. Secondly in dynamic approach, code emulation techniques are used by simulating the processor and operating system to detect suspicious operations (an attempt to open other executable files with the intention of modifying their content, changing the Master Boot Record, concealing themselves from the operating system, etc.) on a virtual machine. The Heuristic method is a promising technique for the detection of unknown malware, in particular to detect encrypted malwares. However, it requires entire virtual environments to be installed. Also it is prone to false alarms, which may make the system more vulnerable by taking the real malware as another false alarm. Researchers augment the results of detection techniques and combine it with another detection technique to reduce the false alarms \cite{Sharma_2014}.

Heuristic is computing is defined as "Proceeding to a solution by trial and error or by rules that are only loosely defined". The main idea behind heuristic based detection is that there is no need to know so much about the inner structure or logic of the program being scanned and the main aim is to reach as close to a conclusive decision as possible using the best optimal path. Therefore, heuristic based detection approaches use algorithms and or rules that scan for known patterns. The first heuristic approaches are known to have been built in 1989 to detect \textit{DOS} viruses. Most antivirus programs today use a combination of heuristic engines and signature based scanners. Recent research in the use of data-mining for malware detection are considered heuristic based detection approaches \cite{NamanyaTWM}.

\color{Black}
\section{Machine Learning}
\color{Red}

In recent years, malware detection with machine learning techniques is gaining popularity. Tom Mitchell \cite{} defines machine learning as the study of computer algorithms that improve through experiments. Robert Moskovitch et. al. \cite{} proposed detection of malwares based on monitoring the computer behaviour (features). His evaluation results suggest that by using classification algorithms applied on only 20 features the mean detection accuracy exceeded 90\%. The advantage of machine learning techniques is that it will not only detect known malwares but also act as knowledge for the detection of new malware. The popular machine learning techniques among the researchers for the detection of second generation malwares are Naive Bayes, Decision Trees, Data Mining, Neural Networks and Hidden Markov Modes. This technique may not replace the standard detection methods, but it can act as an add-on feature. Generally, machine learning techniques are more computationally demanding than the standard anti-malware, hence it may not be suitable for end users. However, it can be implemented at enterprise gateaway level to act as a central anti-malware engine to supplement anti-malwares. Although infrastructure requirement is costly, it can help in protecting valuable enterprises data from the security threats and can prevent immense financial damages \cite{Sharma_2014}.

In recent years, machine learning has gained its popularity in many fields including IT security. Robert Moskovitch et. al. \cite{} proposed a technique that monitors a small set of features that are sufficient for detecting malware without sacrificing accuracy. The result of the study showed that, using only 20 features, the mean detection accuracy was greater than 90\%, and for specific unknown worms, this accuracy got over 99\%, while maintaining a low level of false positives. The advantage of machine learning techniques is that it will not only detect a known malware but also act as a database for detecting new malware. Similar studies can also be found in another model such as Naive Bayes, Decision Trees, Neural Networks. Although this technique is practical, it may not replace the standard detection methods, rather than act as an add-on feature because machine learning techniques are computationally demanding and may not be suitable for end users \cite{NguyenSPVD}.

\color{Black}
\subsection{Simple malware detection network}
\color{Red}

\color{Black}
\subsection{AlOHA: Auxiliary Loss Optimization for Hypothesis Augmentation}
\color{Red}

\color{Black}
\subsection{Automatic Malware Description via Attribute Tagging and Similarity Embedding}
\color{Red}

\color{Black}
\subsection{Learning from Context: Exploiting and Interpreting File Path Information for Better Malware Detection}
\color{Red}

\color{Black}
\section{Malware Normalization}
\color{Red}

The malwares generated from advanced toolkits such as the packer UPX and Mitsfall are difficult to detect. For the detection of such malwares, normalization techniques can be used to improve the detection rate of an existing anti-malware. In this technique, a normalizer accepts the obfuscated version of the malware and eliminates the obfuscation carried on the program and produces the normalized executable. After normalization the signature of the malware is extracted and compared with the signature in canonical form. Chistodorescu et. al. \cite{} designed a malware normalizer that handles three common obfuscation viz. code reordering, packing and junk code insertion. Later on Armor et. al. \cite{}, proposed a generalized malware normalizer which can store obfuscation methods in the form of automata structures and use them for normalizing the metamorphic malwares. Recently a general malaware normalizer has been proposed that can store lots of obfuscation methods in the form of automata structures for normalizing metamorphic malwares, which as a detection rate up to 81\% \cite{Sharma_2014}.

\color{Black}
\chapter{Dataset Used}

\chapter{Experimenting with ML based Malware Detection/Description methods}

\chapter{Proposed Tool}

Description of the proposed tool..

\chapter{Experiments}

\chapter{Results}

Results analysis..

\chapter{Conclusions}

Qui si inseriscono brevi conclusioni sul lavoro svolto, senza ripetere inutilmente il sommario.

Si possono evidenziare i punti di forza e quelli di debolezza, nonché i possibili sviluppi futuri o attività da svolgere per migliorare i risultati.

% bibliografia scritta "a mano"
%\input{biblio.tex}

% se la bibliografia è stata scritta (usando Bibtex) nel file biblio.bib allora commentare la riga precedente e scommentare le due righe seguenti
\bibliographystyle{torsec}
\bibliography{biblio}


\end{document}
